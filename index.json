[{"authors":["admin"],"categories":null,"content":"I am a first year PhD student in Epidemiology and Biostatistics at Tulane University. My research interests lie at the intersection of epidemiological methods, causal inference, nutrition, and cardiometabolic kidney diseases.\nI am a Mississippian who loves his partner, retriever, and being generally skeptical about things.\n","date":1554595200,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I am a first year PhD student in Epidemiology and Biostatistics at Tulane University. My research interests lie at the intersection of epidemiological methods, causal inference, nutrition, and cardiometabolic kidney diseases.\nI am a Mississippian who loves his partner, retriever, and being generally skeptical about things.","tags":null,"title":"Walker Blackston","type":"authors"},{"authors":null,"categories":null,"content":" Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":" In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":" Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature.   Slides can be added in a few ways:\n Create slides using Academic\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"96344c08df50a1b693cc40432115cbe3","permalink":"/talk/example/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example/","section":"talk","summary":"An example talk using Academic's Markdown slides feature.","tags":[],"title":"Example Talk","type":"talk"},{"authors":[],"categories":[],"content":" Top Coffee Near You: Too often, we are confronted with the paradox of choice for delicious things. Do I go here for buttery croissants but crappy coffee? Do I hit up this swanky joint with dope espresso, but at the expense of nay any food items and perhaps a small percentage of my student loan?\nThis program can help.\nEvery time I go to a new place, I sift all outlets for the best coffee (reddit, yelp, carrier pigeon, etc.) but this can be a lengthy and draining process. Why not have a program that you can type in your location and receive just the top three shops, according to average yelp review?\nLet\u0026rsquo;s give it a shot\n#import relevant packages from bs4 import BeautifulSoup import requests, bs4, csv  First, we\u0026rsquo;d like to specify your location and store it for our parser:\nlocation = input(\u0026quot;Where y'at?: \u0026quot;) soup = requests.get(\u0026quot;https://www.yelp.com/search?find_desc=Coffee+Shops\u0026amp;find_loc=\u0026quot; + location).text print('To confirm, you want to look at ' + location + \u0026quot;?\u0026quot;)  Where y'at?: New Orleans To confirm, you want to look at New Orleans?  conf = input(\u0026quot;Confirm Y or N: \u0026quot;) if conf == 'N'or conf == 'n': print(\u0026quot;Let's try this again shall we?\u0026quot;) location = input(\u0026quot;New Location: \u0026quot;) elif conf == 'Y' or conf == 'y': print('Good coffee incoming!') else: print('Confirm with a Yes or No only, you wild person.') #input a Y or N below:  Confirm Y or N: Y Good coffee incoming!  Solid. Now that you have picked and confirmed your location, let\u0026rsquo;s get that beautiful soup brewing. I am still learning this functionality in Python, so it may time some time and refactoring to get everything right, but for now, I will import other uses and adapt it.\nresults = soup.find('h3', string='All Results').find_parent('li').find_next_siblings('li') len(results) #check the length of results  ------------------------------------------------------------------------ TypeError Traceback (most recent call last) \u0026lt;ipython-input-13-985d6118bc72\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 results = soup.find('h3', string='All Results').find_parent('li').find_next_siblings('li') 2 len(results) #check the length of results TypeError: find() takes no keyword arguments  This soup function allows us to go through the argument type \u0026lsquo;h3\u0026rsquo; of an HTML, find the string \u0026lsquo;All Results\u0026rsquo; since we are not interested in sponsored content, then looks upward and across the HTML code trees using \u0026lsquo;.find_parent\u0026rsquo; and \u0026lsquo;.find_next_siblings\u0026rsquo; for other instances of the link tag.\n#print(soup.prettify()) #uncomment to see the full elemental print of the site in HTML tabl_ratings = soup.find('div', attrs = {'class': 'lemon--'}) for row in tabl_ratings.findAll('h3', attrs = {'class': 'All Results'}): coffee = {} coffee['shop name'] = row.a['target name'] coffee['rating'] = row.div['aria-label'] coffee.append(coffee) top_3 = {} # Top 3 places for coffee, stored as a dictionary  ","date":1565952286,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565952286,"objectID":"fb638786e3558012e188113ea08acd6d","permalink":"/post/topcoffee/","publishdate":"2019-08-16T11:44:46+01:00","relpermalink":"/post/topcoffee/","section":"post","summary":"Top Coffee Near You: Too often, we are confronted with the paradox of choice for delicious things. Do I go here for buttery croissants but crappy coffee? Do I hit up this swanky joint with dope espresso, but at the expense of nay any food items and perhaps a small percentage of my student loan?\nThis program can help.\nEvery time I go to a new place, I sift all outlets for the best coffee (reddit, yelp, carrier pigeon, etc.","tags":[],"title":"Topcoffee","type":"post"},{"authors":[],"categories":[],"content":" The State of Gun Violence in New Orleans New Orleans, Louisiana, my dearest hometown, has a complicated relationship with the world. Many come here to party or test their cajun linguistic ability. Maybe you come for the food? Regardless, many are also aware of its issues with public safety. I am looking to test the hypothesis that New Orleans is or is not safe, with open sourced data on gun violence as a proxy for this notion of safety (the Gun Violence Archive or GVA: https://www.gunviolencearchive.org). Data range from January 21, 2013 to March 30, 2018.\nAll thoughts are my own, and these data are open. Please feel free to question, test, replicate and challenge my analytic approach with your own work. As I am very much in the learning phase, please excuse any over-commenting and mistakes and/or clunky code. Thanks, JWB\nStep 0: Packages and Importing Data #Importing all packages and data: %matplotlib inline import numpy as np import pandas as pd import matplotlib import matplotlib.pyplot as plt import seaborn as sb from sklearn.preprocessing import MinMaxScaler  file = \u0026quot;/Users/walker/Desktop/Python/Data/stage3.csv\u0026quot; gv_df = pd.read_csv(file, encoding = 'utf-8') #print the shape of the data to get observations and variables (rows, columns) gv_df.shape gv_df.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    incident_id date state city_or_county address n_killed n_injured incident_url source_url incident_url_fields_missing ... participant_age participant_age_group participant_gender participant_name participant_relationship participant_status participant_type sources state_house_district state_senate_district     0 461105 2013-01-01 Pennsylvania Mckeesport 1506 Versailles Avenue and Coursin Street 0 4 http://www.gunviolencearchive.org/incident/461105 http://www.post-gazette.com/local/south/2013/0... False ... 0::20 0::Adult 18+||1::Adult 18+||2::Adult 18+||3::A... 0::Male||1::Male||3::Male||4::Female 0::Julian Sims NaN 0::Arrested||1::Injured||2::Injured||3::Injure... 0::Victim||1::Victim||2::Victim||3::Victim||4:... http://pittsburgh.cbslocal.com/2013/01/01/4-pe... NaN NaN   1 460726 2013-01-01 California Hawthorne 13500 block of Cerise Avenue 1 3 http://www.gunviolencearchive.org/incident/460726 http://www.dailybulletin.com/article/zz/201301... False ... 0::20 0::Adult 18+||1::Adult 18+||2::Adult 18+||3::A... 0::Male 0::Bernard Gillis NaN 0::Killed||1::Injured||2::Injured||3::Injured 0::Victim||1::Victim||2::Victim||3::Victim||4:... http://losangeles.cbslocal.com/2013/01/01/man-... 62.0 35.0   2 478855 2013-01-01 Ohio Lorain 1776 East 28th Street 1 3 http://www.gunviolencearchive.org/incident/478855 http://chronicle.northcoastnow.com/2013/02/14/... False ... 0::25||1::31||2::33||3::34||4::33 0::Adult 18+||1::Adult 18+||2::Adult 18+||3::A... 0::Male||1::Male||2::Male||3::Male||4::Male 0::Damien Bell||1::Desmen Noble||2::Herman Sea... NaN 0::Injured, Unharmed, Arrested||1::Unharmed, A... 0::Subject-Suspect||1::Subject-Suspect||2::Vic... http://www.morningjournal.com/general-news/201... 56.0 13.0   3 478925 2013-01-05 Colorado Aurora 16000 block of East Ithaca Place 4 0 http://www.gunviolencearchive.org/incident/478925 http://www.dailydemocrat.com/20130106/aurora-s... False ... 0::29||1::33||2::56||3::33 0::Adult 18+||1::Adult 18+||2::Adult 18+||3::A... 0::Female||1::Male||2::Male||3::Male 0::Stacie Philbrook||1::Christopher Ratliffe||... NaN 0::Killed||1::Killed||2::Killed||3::Killed 0::Victim||1::Victim||2::Victim||3::Subject-Su... http://denver.cbslocal.com/2013/01/06/officer-... 40.0 28.0   4 478959 2013-01-07 North Carolina Greensboro 307 Mourning Dove Terrace 2 2 http://www.gunviolencearchive.org/incident/478959 http://www.journalnow.com/news/local/article_d... False ... 0::18||1::46||2::14||3::47 0::Adult 18+||1::Adult 18+||2::Teen 12-17||3::... 0::Female||1::Male||2::Male||3::Female 0::Danielle Imani Jameison||1::Maurice Eugene ... 3::Family 0::Injured||1::Injured||2::Killed||3::Killed 0::Victim||1::Victim||2::Victim||3::Subject-Su... http://myfox8.com/2013/01/08/update-mother-sho... 62.0 27.0    5 rows × 29 columns\n  Looks like our data will include many US cities- not just New Orleans. Also, looks like it contains identifiable characteristics, such as name, that will be scrubbed from further analysis due to irrelevancy.  Step 1: Filter and Clean Data Now that we have our raw data from a .csv, we want to shape it to answer our question specific to New Orleans. This will require finding which column provides the City of the report, then filtering our data by this variable (in this case, \u0026lsquo;city_or_county\u0026rsquo;). I will attempt to filter by the condition of \u0026lsquo;city_or_county\u0026rsquo; being equal to New Orleans.\naside: As I mentioned, I am still learning how to filter data sets in Python. I performed the filtering in R and re-imported a .csv here. The below code was the beginning of my work on this after several attempts.\n#is_nola = gv_nola_df['city_or_county'] == 'New Orleans' #print(is_nola)  #gv_nola_df = gv_nola_df[is_nola]  new_file = \u0026quot;/Users/walker/Desktop/gv_NO.csv\u0026quot; gv_no = pd.read_csv(new_file, encoding = 'utf-8') gv_no.head()   .dataframe tbody tr th:only-of-type { vertical-align: middle; } .dataframe tbody tr th { vertical-align: top; } .dataframe thead th { text-align: right; }    incident_id date state city_or_county address n_killed n_injured incident_url source_url incident_url_fields_missing ... notes participant_age participant_age_group participant_gender participant_relationship participant_status participant_type sources state_house_district state_senate_district     0 479374 1/21/13 Louisiana New Orleans LaSalle Street and Martin Luther King Jr. Boul... 0 5 http://www.gunviolencearchive.org/incident/479374 http://www.nola.com/crime/index.ssf/2013/01/no... False ... Unprovoked drive-by results in multiple teens ... NaN NaN 0::Male||1::Male||2::Male||3::Male||4::Male NaN 0::Injured||1::Injured||2::Injured||3::Injured... 0::Victim||1::Victim||2::Victim||3::Victim||4:... http://www.huffingtonpost.com/2013/01/21/new-o... 93.0 5.0   1 479603 2/9/13 Louisiana New Orleans 400 block of Bourbon Street 0 4 http://www.gunviolencearchive.org/incident/479603 http://www.nola.com/crime/index.ssf/2013/04/su... False ... NaN 0::18||1::22||2::21||3::29||4::19||5::22||6::23 0::Adult 18+||1::Adult 18+||2::Adult 18+||3::A... 0::Male||1::Female||2::Female||3::Male||4::Mal... NaN 0::Injured||1::Injured||2::Injured||3::Injured... 0::Victim||1::Victim||2::Victim||3::Victim||4:... http://www.cbsnews.com/news/mardi-gras-shootin... 93.0 4.0   2 486209 5/12/13 Louisiana New Orleans Frenchmen Street 0 19 http://www.gunviolencearchive.org/incident/486209 http://www.nola.com/crime/index.ssf/2015/09/mo... False ... ms; 19 inj. Rival gang members retaliation eff... 17::10||18::10||19::19||20::23 0::Adult 18+||1::Adult 18+||2::Adult 18+||3::A... 0::Male||1::Male||2::Male||3::Male||4::Male||5... NaN 0::Injured||1::Injured||2::Injured||3::Injured... 0::Victim||1::Victim||2::Victim||3::Victim||4:... http://www.upi.com/Top_News/US/2013/05/12/Abou... 97.0 4.0   3 489121 6/23/13 Louisiana New Orleans 5700 block of Wickfield Drive 0 4 http://www.gunviolencearchive.org/incident/489121 http://www.wwltv.com/story/news/crime/2014/09/... False ... ms. 4 inj at going away party for a soldier in... 0::18||1::20||2::18||3::18 0::Adult 18+||1::Adult 18+||2::Adult 18+||3::A... 0::Female||1::Female||2::Male||3::Male||4::Male NaN 0::Injured||1::Injured||2::Injured||3::Injured... 0::Victim||1::Victim||2::Victim||3::Victim||4:... http://www.nola.com/crime/index.ssf/2013/06/fo... 97.0 3.0   4 92153 1/1/14 Louisiana New Orleans 2300 block of North Prieur Street 1 0 http://www.gunviolencearchive.org/incident/92153 http://www.wwltv.com/news/crime/NOPD-reports-s... False ... NaN NaN 0::Adult 18+ 0::Male NaN 0::Killed 0::Victim http://www.wwltv.com/news/crime/NOPD-reports-s... 93.0 4.0    5 rows × 28 columns\n  Now, we have a filtered data set of:  print(len(gv_no)) #total number of reports len(gv_no)/5 #number of years captured in our dataset range  3153 630.6  observations. So that means from 2013 to 2018, there were roughly 3,153 reports (which equates to roughly 631 per year) of gun violence to some degree in the New Orleans metro area. What does that mean for yearly breakdowns? What of the nature of these reports and what was going on in the country or area during spikes? Lastly, how do these compare to national averages? All of these questions will be the focus of the rest of this report.\n#To plot yearly breakdowns, we need to construct new variables #first inspect the date variable to see if we need to create it in 'datetime' gv_no.date.dtype  dtype('O')  import datetime gv_no['date'] = gv_no['date'].astype('datetime64[ns]') gv_no.date.dtype  dtype('\u0026lt;M8[ns]')  \u0026lsquo;\u0026lt;M8[ns]\u0026rsquo; implies conversion to readable date type\n#TODO: CREATE DICTIONARIES TO HOUSE YEARLY TOTALS ... year_1 = {'killed':0, 'injured':0, 'total':0, '2013 Average':0} year_2 = {'killed':0, 'injured':0, 'total':0, '2014 Average':0} year_3 = {'killed':0, 'injured':0, 'total':0, '2015 Average':0} year_4 = {'killed':0, 'injured':0, 'total':0, '2016 Average':0} year_5 = {'killed':0, 'injured':0, 'total':0, '2017 Average':0} year_6 = {'killed':0, 'injured':0, 'total':0, '2018 Average':0} if 13 in gv_no['date'] and gv_no['n_killed'] \u0026gt; 0: year_1['killed'] +1  ------------------------------------------------------------------------ ValueError Traceback (most recent call last) \u0026lt;ipython-input-13-8f2429ab7160\u0026gt; in \u0026lt;module\u0026gt; 6 year_6 = {'killed':0, 'injured':0, 'total':0, '2018 Average':0} 7 ----\u0026gt; 8 if 13 in gv_no['date'] and gv_no['n_killed'] \u0026gt; 0: 9 year_1['killed'] +1 10 ~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in __nonzero__(self) 1476 raise ValueError(\u0026quot;The truth value of a {0} is ambiguous. \u0026quot; 1477 \u0026quot;Use a.empty, a.bool(), a.item(), a.any() or a.all().\u0026quot; -\u0026gt; 1478 .format(self.__class__.__name__)) 1479 1480 __bool__ = __nonzero__ ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().  print(year_1)  {'killed': 0 0 1 0 2 0 3 0 4 1 5 0 6 0 7 0 8 0 9 0 10 0 11 0 12 0 13 1 14 0 15 0 16 0 17 0 18 0 19 0 20 0 21 0 22 0 23 0 24 0 25 0 26 0 27 0 28 0 29 0 .. 3123 1 3124 0 3125 0 3126 0 3127 1 3128 0 3129 0 3130 0 3131 1 3132 0 3133 1 3134 0 3135 0 3136 0 3137 0 3138 0 3139 2 3140 0 3141 0 3142 1 3143 0 3144 1 3145 0 3146 0 3147 0 3148 1 3149 0 3150 0 3151 0 3152 0 Name: n_killed, Length: 3153, dtype: int64, 'injured': 0 5 1 4 2 19 3 4 4 0 5 2 6 0 7 0 8 1 9 1 10 1 11 1 12 1 13 0 14 0 15 1 16 0 17 1 18 0 19 0 20 0 21 1 22 0 23 0 24 1 25 1 26 0 27 0 28 0 29 1 .. 3123 0 3124 0 3125 0 3126 0 3127 0 3128 0 3129 1 3130 1 3131 0 3132 1 3133 1 3134 1 3135 1 3136 0 3137 1 3138 1 3139 0 3140 1 3141 0 3142 0 3143 0 3144 0 3145 1 3146 2 3147 0 3148 1 3149 1 3150 1 3151 0 3152 1 Name: n_injured, Length: 3153, dtype: int64, 'total': 0, '2013 Average': 0}  ","date":1565952279,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565952279,"objectID":"0bd7ee9a6f8cefe11e37f5758572b216","permalink":"/post/gunviolenceneworleans/","publishdate":"2019-08-16T11:44:39+01:00","relpermalink":"/post/gunviolenceneworleans/","section":"post","summary":"The State of Gun Violence in New Orleans New Orleans, Louisiana, my dearest hometown, has a complicated relationship with the world. Many come here to party or test their cajun linguistic ability. Maybe you come for the food? Regardless, many are also aware of its issues with public safety. I am looking to test the hypothesis that New Orleans is or is not safe, with open sourced data on gun violence as a proxy for this notion of safety (the Gun Violence Archive or GVA: https://www.","tags":[],"title":"Gunviolenceneworleans","type":"post"},{"authors":[],"categories":[],"content":" Noting the suggested ML Workflow, provided by Francois Chollet (2017): These 7 steps are a guaranteed baseline approach to use on any ML project or anlysis:\n1) Define our Problem:  a) What will our data look like? What is our target variable(s)? Will our data be amenable to splitting into training, validation, and test sets?\n b) What tools will we be using? Binary, multi-class, or multi-label classification? Perhaps scalar regression? Unsupervised learning or dimensionality reduction? This is a function of (a) and an in-depth understanding of our data.\n  2) Define \u0026lsquo;success\u0026rsquo; in our problem:  a) What does a successful model look like and how would we assess this? Typically model accuracy through metrics such as ROC-area under curve are useful for classification problems. For regression we might employ MAE metrics.  3) Evaluation Protocol:  Will we use k-fold cross validation? Will we shuffle data prior to splitting into training and test datasets? Also consider \u0026lsquo;hold-out\u0026rsquo; methods for data and imputing dropout layers in our network.  4) Prepare our data:  Format data into tensors, typically scaled to appropriate values (i.e. -1 - 1, or 0 - 1) Normalize features that take many different forms or distributions  5) Training Data, Baseline Model: Main choices here: - a) What will our last activation layer look like? remember that for binary classification, a final \u0026lsquo;sigmoid\u0026rsquo; layer with 1 hidden unit would be appropriate, whereas a scalar regression problem should contain NO final layer of activation, and our multi-classifcation problem required a final layer (\u0026lsquo;softmax\u0026rsquo;) of n-units depending on the features\n b) What loss function should we use? Binary classification = \u0026lsquo;binary_crossentropy\u0026rsquo;; regression = \u0026lsquo;mse\u0026rsquo;; etc.\n c) What optimization measure will be employed to assess the model learning rate? Usually \u0026lsquo;rmsprop\u0026rsquo; is sufficient.\n  *keep in mind that your hypothesis here is that your model will predict better\n6) Get big!  Here, we will incrementally scale up our model, meaning a few key things:  adding layers adding units in those layers train over more epochs    Make sure to plot/monitor training loss and validation loss over these increments!\n7) Optimize Model Parameters/Hyperparameters: Suggested steps to do iteratively:\n Add dropout layers (induce random zeroes etc.) Implement different architectures, ADD or REMOVE layers Try out some different hyperparam\u0026rsquo;s - increase or decrease the units per layer, learning rate of the optimizer etc. Consider some feature engineering, or configure the data for features that are useful, logical, or altogether inofrmative  ","date":1565952262,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565952262,"objectID":"c8e47cae63f6399396a7d64e2a414c4e","permalink":"/post/the-semi-universal-ml-workflow/","publishdate":"2019-08-16T11:44:22+01:00","relpermalink":"/post/the-semi-universal-ml-workflow/","section":"post","summary":"Noting the suggested ML Workflow, provided by Francois Chollet (2017): These 7 steps are a guaranteed baseline approach to use on any ML project or anlysis:\n1) Define our Problem:  a) What will our data look like? What is our target variable(s)? Will our data be amenable to splitting into training, validation, and test sets?\n b) What tools will we be using? Binary, multi-class, or multi-label classification? Perhaps scalar regression?","tags":[],"title":"The Semi Universal Ml Workflow","type":"post"},{"authors":[],"categories":[],"content":" from keras.datasets import reuters (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)  Using TensorFlow backend. WARNING: Logging before flag parsing goes to stderr. W0714 18:24:28.310750 4580939200 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead. W0714 18:24:28.311494 4580939200 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.AttrValue is deprecated. Please use tf.compat.v1.AttrValue instead. W0714 18:24:28.312218 4580939200 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.COMPILER_VERSION is deprecated. Please use tf.version.COMPILER_VERSION instead. W0714 18:24:28.312906 4580939200 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.CXX11_ABI_FLAG is deprecated. Please use tf.sysconfig.CXX11_ABI_FLAG instead. W0714 18:24:28.313544 4580939200 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.ConditionalAccumulator is deprecated. Please use tf.compat.v1.ConditionalAccumulator instead.  len(train_data)  8982  len(test_data)  2246  train_data[5]  [1, 4, 37, 38, 309, 213, 349, 1632, 48, 193, 229, 463, 28, 156, 635, 11, 82, 14, 156, 635, 11, 82, 54, 139, 16, 349, 105, 462, 311, 28, 296, 147, 11, 82, 14, 296, 147, 11, 54, 139, 342, 48, 193, 3234, 361, 122, 23, 1332, 28, 318, 942, 11, 82, 14, 318, 942, 11, 82, 54, 139, 122, 7, 105, 462, 23, 349, 28, 296, 767, 11, 82, 14, 296, 767, 11, 54, 139, 342, 229, 162, 7, 48, 193, 55, 408, 28, 258, 557, 11, 82, 14, 196, 557, 11, 82, 54, 139, 162, 7, 105, 462, 55, 349, 28, 191, 968, 11, 82, 14, 191, 785, 11, 54, 139, 17, 12]  import numpy as np def vectorize_sequences(sequences, dimension=10000): results = np.zeros((len(sequences), dimension)) for i, sequence in enumerate(sequences): results[i, sequence] = 1. return results # Our vectorized training data x_train = vectorize_sequences(train_data) # Our vectorized test data x_test = vectorize_sequences(test_data)  #one-hot encode our categorical variables, the keras way! from keras.utils.np_utils import to_categorical one_hot_train_labels = to_categorical(train_labels) one_hot_test_labels = to_categorical(test_labels)  from keras import models from keras import layers model = models.Sequential() model.add(layers.Dense(64, activation = 'relu', input_shape=(10000, ))) model.add(layers.Dense(64, activation = 'relu')) model.add(layers.Dense(46, activation = 'softmax')) #46 due 46-dimensional output- this will output a probability function for all dimensional spaces added up to 1  model.compile(optimizer='rmsprop', loss = 'categorical_crossentropy', #due to the categorical nature of our target labels metrics = ['accuracy'])  W0714 18:24:36.805853 4580939200 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.  x_val = x_train[:1000] partial_x_train = x_train[1000:] y_val = one_hot_train_labels[:1000] partial_y_train = one_hot_train_labels[1000:]  history = model.fit(partial_x_train, partial_y_train, epochs = 20, batch_size = 512, validation_data = (x_val, y_val))  W0714 18:24:39.409189 4580939200 deprecation.py:323] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1251: add_dispatch_support.\u0026lt;locals\u0026gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where Train on 7982 samples, validate on 1000 samples Epoch 1/20 7982/7982 [==============================] - 1s 139us/step - loss: 2.5322 - acc: 0.4955 - val_loss: 1.7208 - val_acc: 0.6120 Epoch 2/20 7982/7982 [==============================] - 1s 98us/step - loss: 1.4452 - acc: 0.6879 - val_loss: 1.3459 - val_acc: 0.7060 Epoch 3/20 7982/7982 [==============================] - 1s 97us/step - loss: 1.0953 - acc: 0.7651 - val_loss: 1.1708 - val_acc: 0.7430 Epoch 4/20 7982/7982 [==============================] - 1s 98us/step - loss: 0.8697 - acc: 0.8165 - val_loss: 1.0793 - val_acc: 0.7590 Epoch 5/20 7982/7982 [==============================] - 1s 96us/step - loss: 0.7034 - acc: 0.8472 - val_loss: 0.9844 - val_acc: 0.7810 Epoch 6/20 7982/7982 [==============================] - 1s 98us/step - loss: 0.5667 - acc: 0.8802 - val_loss: 0.9411 - val_acc: 0.8040 Epoch 7/20 7982/7982 [==============================] - 1s 98us/step - loss: 0.4581 - acc: 0.9048 - val_loss: 0.9083 - val_acc: 0.8020 Epoch 8/20 7982/7982 [==============================] - 1s 96us/step - loss: 0.3695 - acc: 0.9231 - val_loss: 0.9363 - val_acc: 0.7890 Epoch 9/20 7982/7982 [==============================] - 1s 98us/step - loss: 0.3032 - acc: 0.9315 - val_loss: 0.8917 - val_acc: 0.8090 Epoch 10/20 7982/7982 [==============================] - 1s 97us/step - loss: 0.2537 - acc: 0.9414 - val_loss: 0.9071 - val_acc: 0.8110 Epoch 11/20 7982/7982 [==============================] - 1s 97us/step - loss: 0.2187 - acc: 0.9471 - val_loss: 0.9177 - val_acc: 0.8130 Epoch 12/20 7982/7982 [==============================] - 1s 98us/step - loss: 0.1873 - acc: 0.9508 - val_loss: 0.9027 - val_acc: 0.8130 Epoch 13/20 7982/7982 [==============================] - 1s 97us/step - loss: 0.1703 - acc: 0.9521 - val_loss: 0.9323 - val_acc: 0.8110 Epoch 14/20 7982/7982 [==============================] - 1s 98us/step - loss: 0.1536 - acc: 0.9554 - val_loss: 0.9689 - val_acc: 0.8050 Epoch 15/20 7982/7982 [==============================] - 1s 98us/step - loss: 0.1390 - acc: 0.9560 - val_loss: 0.9686 - val_acc: 0.8150 Epoch 16/20 7982/7982 [==============================] - 1s 99us/step - loss: 0.1313 - acc: 0.9560 - val_loss: 1.0220 - val_acc: 0.8060 Epoch 17/20 7982/7982 [==============================] - 1s 99us/step - loss: 0.1217 - acc: 0.9579 - val_loss: 1.0254 - val_acc: 0.7970 Epoch 18/20 7982/7982 [==============================] - 1s 97us/step - loss: 0.1198 - acc: 0.9582 - val_loss: 1.0430 - val_acc: 0.8060 Epoch 19/20 7982/7982 [==============================] - 1s 96us/step - loss: 0.1138 - acc: 0.9597 - val_loss: 1.0955 - val_acc: 0.7970 Epoch 20/20 7982/7982 [==============================] - 1s 97us/step - loss: 0.1111 - acc: 0.9593 - val_loss: 1.0674 - val_acc: 0.8020  #viz the accuracy and loss across 20 epochs: import matplotlib.pyplot as plt loss = history.history['loss'] val_loss = history.history['val_loss'] epochs = range(1, len(loss) + 1) plt.plot(epochs, loss, 'bo', label='Training loss') plt.plot(epochs, val_loss, 'b', label='Validation loss') plt.title('Training and validation loss') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend() plt.show()  \u0026lt;Figure size 640x480 with 1 Axes\u0026gt;  plt.clf() # clear figure acc = history.history['acc'] val_acc = history.history['val_acc'] plt.plot(epochs, acc, 'bo', label='Training acc') plt.plot(epochs, val_acc, 'b', label='Validation acc') plt.title('Training and validation accuracy') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend() plt.show()  #looks like at around 9 epochs there's an asymptote of both model accuracy and loss #re-train a model with 9 epochs: model = models.Sequential() model.add(layers.Dense(64, activation = 'relu', input_shape=(10000, ))) model.add(layers.Dense(64, activation = 'relu')) model.add(layers.Dense(46, activation = 'softmax')) model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['accuracy']) model.fit(partial_x_train, partial_y_train, epochs=9, #change to 9 epochs from 20 batch_size=512, validation_data=(x_val, y_val)) results = model.evaluate(x_test, one_hot_test_labels)  Train on 7982 samples, validate on 1000 samples Epoch 1/9 7982/7982 [==============================] - 1s 124us/step - loss: 2.8046 - acc: 0.5038 - val_loss: 1.8434 - val_acc: 0.6300 Epoch 2/9 7982/7982 [==============================] - 1s 96us/step - loss: 1.5257 - acc: 0.6957 - val_loss: 1.3305 - val_acc: 0.7120 Epoch 3/9 7982/7982 [==============================] - 1s 95us/step - loss: 1.1115 - acc: 0.7710 - val_loss: 1.1392 - val_acc: 0.7470 Epoch 4/9 7982/7982 [==============================] - 1s 97us/step - loss: 0.8719 - acc: 0.8146 - val_loss: 1.0308 - val_acc: 0.7850 Epoch 5/9 7982/7982 [==============================] - 1s 97us/step - loss: 0.7032 - acc: 0.8475 - val_loss: 0.9681 - val_acc: 0.8020 Epoch 6/9 7982/7982 [==============================] - 1s 96us/step - loss: 0.5661 - acc: 0.8753 - val_loss: 0.9420 - val_acc: 0.7980 Epoch 7/9 7982/7982 [==============================] - 1s 95us/step - loss: 0.4567 - acc: 0.9074 - val_loss: 0.9235 - val_acc: 0.7960 Epoch 8/9 7982/7982 [==============================] - 1s 95us/step - loss: 0.3709 - acc: 0.9238 - val_loss: 0.9698 - val_acc: 0.7920 Epoch 9/9 7982/7982 [==============================] - 1s 95us/step - loss: 0.3020 - acc: 0.9361 - val_loss: 0.8983 - val_acc: 0.8150 2246/2246 [==============================] - 0s 61us/step  results  [0.979011650076957, 0.7894033837403343]  #verify predictions on novel data predictions = model.predict(x_test)  predictions[0].shape  (46,)  np.sum(predictions[0])  1.0000001  Experiment with different Network Layers and Units: model_128u = models.Sequential() model_128u.add(layers.Dense(128, activation='relu', input_shape=(10000,))) model_128u.add(layers.Dense(128, activation='relu')) model_128u.add(layers.Dense(46, activation = 'softmax')) model_128u.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) hist_128u = model_128u.fit(partial_x_train, partial_y_train, epochs=20, batch_size = 512, validation_data = (x_val, y_val))  Train on 7982 samples, validate on 1000 samples Epoch 1/20 7982/7982 [==============================] - 1s 144us/step - loss: 2.1379 - acc: 0.5540 - val_loss: 1.3870 - val_acc: 0.6880 Epoch 2/20 7982/7982 [==============================] - 1s 86us/step - loss: 1.0956 - acc: 0.7648 - val_loss: 1.0949 - val_acc: 0.7690 Epoch 3/20 7982/7982 [==============================] - 1s 117us/step - loss: 0.7737 - acc: 0.8322 - val_loss: 0.9575 - val_acc: 0.8150 Epoch 4/20 7982/7982 [==============================] - 1s 118us/step - loss: 0.5631 - acc: 0.8834 - val_loss: 0.9109 - val_acc: 0.8090 Epoch 5/20 7982/7982 [==============================] - 1s 118us/step - loss: 0.4036 - acc: 0.9158 - val_loss: 0.8710 - val_acc: 0.8150 Epoch 6/20 7982/7982 [==============================] - 1s 117us/step - loss: 0.3238 - acc: 0.9315 - val_loss: 0.8674 - val_acc: 0.8220 Epoch 7/20 7982/7982 [==============================] - 1s 117us/step - loss: 0.2422 - acc: 0.9458 - val_loss: 0.8880 - val_acc: 0.8220 Epoch 8/20 7982/7982 [==============================] - 1s 118us/step - loss: 0.2104 - acc: 0.9486 - val_loss: 0.9060 - val_acc: 0.8190 Epoch 9/20 7982/7982 [==============================] - 1s 112us/step - loss: 0.1774 - acc: 0.9518 - val_loss: 0.9197 - val_acc: 0.8100 Epoch 10/20 7982/7982 [==============================] - 1s 117us/step - loss: 0.1621 - acc: 0.9525 - val_loss: 0.9367 - val_acc: 0.8100 Epoch 11/20 7982/7982 [==============================] - 1s 115us/step - loss: 0.1456 - acc: 0.9555 - val_loss: 0.9720 - val_acc: 0.8130 Epoch 12/20 7982/7982 [==============================] - 1s 95us/step - loss: 0.1337 - acc: 0.9578 - val_loss: 1.0050 - val_acc: 0.7970 Epoch 13/20 7982/7982 [==============================] - 1s 118us/step - loss: 0.1357 - acc: 0.9557 - val_loss: 0.9884 - val_acc: 0.8150 Epoch 14/20 7982/7982 [==============================] - 1s 116us/step - loss: 0.1292 - acc: 0.9562 - val_loss: 1.0644 - val_acc: 0.7970 Epoch 15/20 7982/7982 [==============================] - 1s 112us/step - loss: 0.1193 - acc: 0.9573 - val_loss: 1.0271 - val_acc: 0.8030 Epoch 16/20 7982/7982 [==============================] - 1s 117us/step - loss: 0.1165 - acc: 0.9573 - val_loss: 1.0800 - val_acc: 0.7970 Epoch 17/20 7982/7982 [==============================] - 1s 118us/step - loss: 0.1181 - acc: 0.9569 - val_loss: 1.0277 - val_acc: 0.7950 Epoch 18/20 7982/7982 [==============================] - 1s 118us/step - loss: 0.1093 - acc: 0.9572 - val_loss: 1.1501 - val_acc: 0.7920 Epoch 19/20 7982/7982 [==============================] - 1s 117us/step - loss: 0.1085 - acc: 0.9573 - val_loss: 1.0522 - val_acc: 0.8040 Epoch 20/20 7982/7982 [==============================] - 1s 87us/step - loss: 0.1081 - acc: 0.9578 - val_loss: 1.0899 - val_acc: 0.7930  plt.clf() # clear figure acc = hist_128u.history['acc'] val_acc = hist_128u.history['val_acc'] plt.plot(epochs, acc, 'bo', label='Training acc') plt.plot(epochs, val_acc, 'b', label='Validation acc') plt.title('Training and validation accuracy') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend() plt.show()  #looks like we see the peak of our asymptote at around 7 epochs: model_128u = models.Sequential() model_128u.add(layers.Dense(128, activation='relu', input_shape=(10000,))) model_128u.add(layers.Dense(128, activation='relu')) model_128u.add(layers.Dense(46, activation = 'softmax')) model_128u.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy']) hist_128u = model_128u.fit(partial_x_train, partial_y_train, epochs=7, batch_size = 512, validation_data = (x_val, y_val)) result_128u = model_128u.evaluate(x_test, one_hot_test_labels)  Train on 7982 samples, validate on 1000 samples Epoch 1/7 7982/7982 [==============================] - 1s 151us/step - loss: 2.1407 - acc: 0.5378 - val_loss: 1.3850 - val_acc: 0.6630 Epoch 2/7 7982/7982 [==============================] - 1s 110us/step - loss: 1.0999 - acc: 0.7590 - val_loss: 1.0990 - val_acc: 0.7700 Epoch 3/7 7982/7982 [==============================] - 1s 117us/step - loss: 0.7843 - acc: 0.8310 - val_loss: 0.9611 - val_acc: 0.7960 Epoch 4/7 7982/7982 [==============================] - 1s 117us/step - loss: 0.5626 - acc: 0.8771 - val_loss: 0.9129 - val_acc: 0.8140 Epoch 5/7 7982/7982 [==============================] - 1s 119us/step - loss: 0.4093 - acc: 0.9154 - val_loss: 0.8941 - val_acc: 0.8050 Epoch 6/7 7982/7982 [==============================] - 1s 104us/step - loss: 0.3111 - acc: 0.9332 - val_loss: 0.9658 - val_acc: 0.7890 Epoch 7/7 7982/7982 [==============================] - 1s 117us/step - loss: 0.2485 - acc: 0.9427 - val_loss: 0.9028 - val_acc: 0.8060 2246/2246 [==============================] - 0s 49us/step  result_128u  [0.9940730629493801, 0.786286731967943]  ","date":1565952246,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565952246,"objectID":"6a1d2e17f81ffbe1f88c784db037280d","permalink":"/post/reuters-headline-classification-model/","publishdate":"2019-08-16T11:44:06+01:00","relpermalink":"/post/reuters-headline-classification-model/","section":"post","summary":"from keras.datasets import reuters (train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)  Using TensorFlow backend. WARNING: Logging before flag parsing goes to stderr. W0714 18:24:28.310750 4580939200 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead. W0714 18:24:28.311494 4580939200 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.AttrValue is deprecated. Please use tf.compat.v1.AttrValue instead. W0714 18:24:28.312218 4580939200 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.COMPILER_VERSION is deprecated. Please use tf.version.COMPILER_VERSION instead. W0714 18:24:28.312906 4580939200 deprecation_wrapper.","tags":[],"title":"Reuters Headline Classification Model","type":"post"},{"authors":[],"categories":[],"content":" #from page 78 of DL (Chollet) from keras.datasets import boston_housing (train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()  Using TensorFlow backend. WARNING: Logging before flag parsing goes to stderr. W0715 11:25:49.465377 4610311616 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead. W0715 11:25:49.466117 4610311616 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.AttrValue is deprecated. Please use tf.compat.v1.AttrValue instead. W0715 11:25:49.466736 4610311616 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.COMPILER_VERSION is deprecated. Please use tf.version.COMPILER_VERSION instead. W0715 11:25:49.467211 4610311616 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.CXX11_ABI_FLAG is deprecated. Please use tf.sysconfig.CXX11_ABI_FLAG instead. W0715 11:25:49.468060 4610311616 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.ConditionalAccumulator is deprecated. Please use tf.compat.v1.ConditionalAccumulator instead.  train_data.shape  (404, 13)  #this process is called feature-wise normalization: mean = train_data.mean(axis=0) train_data -= mean std = train_data.std(axis=0) train_data/= std test_data -= mean test_data /= std  This centers the means around 0 and provides a unit standard deviation\nfrom keras import layers from keras import models def build_model(): model = models.Sequential() model.add(layers.Dense(64, activation='relu', input_shape=(train_data.shape[1],))) model.add(layers.Dense(64, activation='relu')) model.add(layers.Dense(1)) model.compile(optimizer='rmsprop', loss='mse', metrics=['mae']) return model  1) Note: no activation on this layer- typical for scalar regression problems, this allows our network to learn values of any range (rather than, say \u0026lsquo;sigmoid\u0026rsquo; to predict outputs between 0 and 1)\n2) Note: \u0026lsquo;mae\u0026rsquo; stands for mean absolute error, or the absolute difference between the preds and the targets - An MAE = 0.5 here means that our predicted house price values would be off by $500\nIn our k-fold cross validation, we need to be wise about how we split our training sets: Since these clusters could be relatively small, we need to group by thpose data with the highest variance\n K-fold validation implies splitting our data into some k= number of clusters of data, running K identical models, then spitting out an averaged score of performance across models  #k val set-up: import numpy as np k = 4 num_val_samples = len(train_data) // k num_epochs = 100 all_scores = [] for i in range(k): print('processing fold #', i) # Prepare the validation data: data from partition # k val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples] val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples] # Prepare the training data: data from all other partitions partial_train_data = np.concatenate( [train_data[:i * num_val_samples], train_data[(i + 1) * num_val_samples:]], axis=0) partial_train_targets = np.concatenate( [train_targets[:i * num_val_samples], train_targets[(i + 1) * num_val_samples:]], axis=0) # Build the Keras model (already compiled) model = build_model() # Train the model (in silent mode, verbose=0) model.fit(partial_train_data, partial_train_targets, epochs=num_epochs, batch_size=1, verbose=0) # Evaluate the model on the validation data val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0) all_scores.append(val_mae)  W0715 11:28:34.739506 4610311616 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead. processing fold # 0 processing fold # 1 processing fold # 2 processing fold # 3  all_scores  [2.4400245529590268]  np.mean(all_scores)  2.4400245529590268  This means that we are off by about $2,440 in our prediction of price. Not insignificant for housing costs in 1970\u0026hellip; Let\u0026rsquo;s see what we can do to improve:\nnum_epochs = 500 all_mae_histories = [] for i in range(k): print('processing fold #', i) # Prepare the validation data: data from partition # k val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples] val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples] # Prepare the training data: data from all other partitions partial_train_data = np.concatenate( [train_data[:i * num_val_samples], train_data[(i + 1) * num_val_samples:]], axis=0) partial_train_targets = np.concatenate( [train_targets[:i * num_val_samples], train_targets[(i + 1) * num_val_samples:]], axis=0) # Build the Keras model (already compiled) model = build_model() # Train the model (in silent mode, verbose=0) history = model.fit(partial_train_data, partial_train_targets, validation_data=(val_data, val_targets), epochs=num_epochs, batch_size=1, verbose=0) mae_history = history.history['val_mean_absolute_error'] all_mae_histories.append(mae_history)  processing fold # 0 processing fold # 1 processing fold # 2 processing fold # 3  average_mae_history = [ np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]  %matplotlib inline #plotting mae history: import matplotlib.pyplot as plt plt.plot(range(1, len(average_mae_history) + 1), average_mae_history) #plots the length or number of average maes, adds one and continues to plot plt.xlabel('Epochs') plt.ylabel('Val MAE') plt.show()  def smooth_curve(points, factor=0.9): smoothed_points = [] for point in points: if smoothed_points: previous = smoothed_points[-1] smoothed_points.append(previous * factor + point * (1 - factor)) else: smoothed_points.append(point) return smoothed_points smooth_mae_history = smooth_curve(average_mae_history[10:]) plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history) plt.xlabel('Epochs') plt.ylabel('Validation MAE') plt.show()  #let's retrain data: model = build_model() #on entire data set: model.fit(train_data, train_targets, epochs = 80, batch_size=16, verbose=0) test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)  102/102 [==============================] - 0s 329us/step  test_mae_score  2.9702411632911834  Off by now almost $3,000! - analysis and modeling effort to be continued\n","date":1565952236,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565952236,"objectID":"8291718af11da09bbdad37de5495385a","permalink":"/post/predicting-housing-prices-example/","publishdate":"2019-08-16T11:43:56+01:00","relpermalink":"/post/predicting-housing-prices-example/","section":"post","summary":"#from page 78 of DL (Chollet) from keras.datasets import boston_housing (train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()  Using TensorFlow backend. WARNING: Logging before flag parsing goes to stderr. W0715 11:25:49.465377 4610311616 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead. W0715 11:25:49.466117 4610311616 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.AttrValue is deprecated. Please use tf.compat.v1.AttrValue instead. W0715 11:25:49.466736 4610311616 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.COMPILER_VERSION is deprecated. Please use tf.","tags":[],"title":"Predicting Housing Prices Example","type":"post"},{"authors":[],"categories":[],"content":"import deepChem as dc import numpy as np  --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) \u0026lt;ipython-input-1-cbeda8b5e316\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 import deepChem as dc 2 import numpy as np ModuleNotFoundError: No module named 'deepChem'  ","date":1565952218,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565952218,"objectID":"b201148421dc0813631a4dfa499097fa","permalink":"/post/jund-transcriptional-factor-binding-site-prediction/","publishdate":"2019-08-16T11:43:38+01:00","relpermalink":"/post/jund-transcriptional-factor-binding-site-prediction/","section":"post","summary":"import deepChem as dc import numpy as np  --------------------------------------------------------------------------- ModuleNotFoundError Traceback (most recent call last) \u0026lt;ipython-input-1-cbeda8b5e316\u0026gt; in \u0026lt;module\u0026gt; ----\u0026gt; 1 import deepChem as dc 2 import numpy as np ModuleNotFoundError: No module named 'deepChem'  ","tags":[],"title":"Jund Transcriptional Factor Binding Site Prediction","type":"post"},{"authors":[],"categories":[],"content":" Predicting Movie Ratings in Keras - Model Tuning:  We trained a neural network with 4 layers on the Keras IMDB data set in order to predict movie ratings from random training data We reached a local minimum of validation loss and model accuracy of 17% and 94%, respectively.\n We posited a change in:\n  1) Number of tensors/layers\n2) Units per tensor\n3) Parameters of model evaluation/activation that may improve our model\u0026rsquo;s predictive power.\nWe will address each of these in turn, as fitting our models first will allow subsequent incremental changes (rather than having to re-run models each time, which can get computationally expensive).\n1) Tweaking hidden layers: This sounds more exotic than it is. We simply want to add and remove hidden layers to our initial model, monitoring changes in performance along the way.\nFirst, we need to re-import all of our data and pre-processing steps to ensure reproducibility:\nfrom keras.datasets import imdb (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000) #we need to vectorize our data: import numpy as np def vect_seq(sequences, dimension=10000): # Create an all-zero matrix of shape (len(sequences), dimension) results = np.zeros((len(sequences), dimension)) for i, sequence in enumerate(sequences): results[i, sequence] = 1. # set specific indices of results[i] to 1s return results # Our vectorized training data x_train = vect_seq(train_data) # Our vectorized test data x_test = vect_seq(test_data) y_train = np.asarray(train_labels).astype('float32') y_test = np.asarray(test_labels).astype('float32')  Using TensorFlow backend. WARNING: Logging before flag parsing goes to stderr. W0714 12:34:16.196576 4322899392 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead. W0714 12:34:16.197201 4322899392 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.AttrValue is deprecated. Please use tf.compat.v1.AttrValue instead. W0714 12:34:16.197646 4322899392 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.COMPILER_VERSION is deprecated. Please use tf.version.COMPILER_VERSION instead. W0714 12:34:16.198417 4322899392 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.CXX11_ABI_FLAG is deprecated. Please use tf.sysconfig.CXX11_ABI_FLAG instead. W0714 12:34:16.198928 4322899392 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.ConditionalAccumulator is deprecated. Please use tf.compat.v1.ConditionalAccumulator instead.  from keras import models from keras import layers from keras import optimizers from keras import losses from keras import metrics import matplotlib.pyplot as plt  Model with n+1 layers:\nmod_3L = models.Sequential() mod_3L.add(layers.Dense(16, activation='relu', input_shape=(10000,))) mod_3L.add(layers.Dense(16, activation='relu')) mod_3L.add(layers.Dense(16, activation='relu')) #additional dense hidden layer with 16 units mod_3L.add(layers.Dense(1, activation='sigmoid'))  #compile: mod_3L.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])  W0714 12:34:28.305108 4322899392 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead. W0714 12:34:28.324115 4322899392 deprecation.py:323] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:182: add_dispatch_support.\u0026lt;locals\u0026gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.where in 2.0, which has the same broadcast rule as np.where  mod_3L.fit(x_train, y_train, epochs=4, batch_size=512) results_3L = mod_3L.evaluate(x_test, y_test)  Epoch 1/4 25000/25000 [==============================] - 2s 80us/step - loss: 0.1382 - acc: 0.9502 Epoch 2/4 25000/25000 [==============================] - 2s 79us/step - loss: 0.1224 - acc: 0.9568 Epoch 3/4 25000/25000 [==============================] - 2s 79us/step - loss: 0.1047 - acc: 0.9622 Epoch 4/4 25000/25000 [==============================] - 2s 79us/step - loss: 0.0892 - acc: 0.9690 25000/25000 [==============================] - 1s 47us/step  results_3L  [0.42676588655471803, 0.86348]  mod_3L.predict(x_test)  array([[0.0731495 ], [0.99997604], [0.5084689 ], ..., [0.09003073], [0.01447853], [0.7448582 ]], dtype=float32)   Looks like we have a moderate loss of model accuracy, with a wider range of predictive point estimates for individual reviews. We can conclude adding a hidden layer does not likely improve this model.  1.1. Removing a hidden layer: mod_1L = models.Sequential() mod_1L.add(layers.Dense(16, activation='relu', input_shape=(10000,))) #just 1 dense, 16 unit layer mod_1L.add(layers.Dense(1, activation='sigmoid'))  mod_1L.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])  mod_1L.fit(x_train, y_train, epochs=4, batch_size=512) results_1L = mod_1L.evaluate(x_test, y_test)  Epoch 1/4 25000/25000 [==============================] - 2s 88us/step - loss: 0.1713 - acc: 0.9412 Epoch 2/4 25000/25000 [==============================] - 2s 84us/step - loss: 0.1531 - acc: 0.9486 Epoch 3/4 25000/25000 [==============================] - 2s 84us/step - loss: 0.1388 - acc: 0.9545 Epoch 4/4 25000/25000 [==============================] - 2s 84us/step - loss: 0.1274 - acc: 0.9574 25000/25000 [==============================] - 1s 30us/step  results_1L  [0.3138607511281967, 0.8792]  mod_1L.predict(x_test)  array([[0.15594196], [0.9999641 ], [0.70080477], ..., [0.12436602], [0.04793406], [0.61118674]], dtype=float32)   Little to no impact to our model accuracy, however removing n=1 dense layer significantly reduced the variance (at least superficially), between the highest and lowest predictive accuracy of our neural net on individual reviews.  Looks like changing our layers has little to no impact on our overall model accuracy. Prioritizing simplicity, lets stick to our \u0026lsquo;mod_1L\u0026rsquo; with one dense layer, and now tweak tensor units: 2) Unit-per-Layer Differentiation: A fancy way to phrase \u0026lsquo;we will now tweak the number of hidden units will be present in each layer\u0026rsquo;\nmod_1L_32u = models.Sequential() mod_1L_32u.add(layers.Dense(32, activation='relu', input_shape=(10000, ))) mod_1L_32u.add(layers.Dense(1, activation='sigmoid'))  mod_1L_32u.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])  mod_1L_32u.fit(x_train, y_train, epochs=4, batch_size=512) results_1L_32u = mod_1L_32u.evaluate(x_test, y_test)  Epoch 1/4 25000/25000 [==============================] - 2s 92us/step - loss: 0.4251 - acc: 0.8314 Epoch 2/4 25000/25000 [==============================] - 2s 86us/step - loss: 0.2520 - acc: 0.9122 Epoch 3/4 25000/25000 [==============================] - 2s 86us/step - loss: 0.1993 - acc: 0.9292 Epoch 4/4 25000/25000 [==============================] - 2s 86us/step - loss: 0.1696 - acc: 0.9422 25000/25000 [==============================] - 1s 32us/step  results_1L_32u  [0.2895566564083099, 0.88496]   *Interesting. We see some general improvement to our model accuracy (around 2%). We could stop, here, but let\u0026rsquo;s exhaust all possibilities to see if this is just a local maximum of accuracy or global. Can we improve further\u0026hellip;?*  2.1. 64 Unit Hidden Layers: mod_1L_64u = models.Sequential() mod_1L_64u.add(layers.Dense(32, activation='relu', input_shape=(10000, ))) mod_1L_64u.add(layers.Dense(1, activation='sigmoid'))  mod_1L_64u.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])  mod_1L_64u.fit(x_train, y_train, epochs=4, batch_size=512) results_1L_64u = mod_1L_64u.evaluate(x_test, y_test)  Epoch 1/4 25000/25000 [==============================] - 2s 93us/step - loss: 0.4199 - acc: 0.8327 Epoch 2/4 25000/25000 [==============================] - 2s 86us/step - loss: 0.2522 - acc: 0.9114 Epoch 3/4 25000/25000 [==============================] - 2s 86us/step - loss: 0.1995 - acc: 0.9303 Epoch 4/4 25000/25000 [==============================] - 2s 86us/step - loss: 0.1693 - acc: 0.9424 25000/25000 [==============================] - 1s 32us/step  results_1L_64u  [0.2949124105834961, 0.88192]  mod_1L_64u.predict(x_test)  array([[0.19108787], [0.99987483], [0.7720454 ], ..., [0.10865086], [0.06636879], [0.51513207]], dtype=float32)   Little different from our previous network. Let\u0026rsquo;s stick to our \u0026lsquo;simpler is better\u0026rsquo; heuristic, thereby sticking to 32 units within 1 hidden layer.\n Now, we can tweak different activation parameters and monitor changes as before. Let\u0026rsquo;s do this now:\n  3) Model Compiling and Activation Parameters: *First, we evaluate our model using a different loss function. To avoid the technical details, this is the function that will specify how we descend down our gradient for tensor weights.\nTo be even less technical, changing this parameter changes how our network \u0026lsquo;learns\u0026rsquo; and updates our models.\n#chaning our compiler for mod_1L_32u to an 'MSE' or mean squared error loss function: mod_1L_32u.compile(optimizer='rmsprop', loss='mse', metrics=['accuracy'])  mod_1L_32u.fit(x_train, y_train, epochs=4, batch_size=512) results_1L_32u_fit2 = mod_1L_32u.evaluate(x_test, y_test)  Epoch 1/4 25000/25000 [==============================] - 2s 92us/step - loss: 0.0432 - acc: 0.9461 Epoch 2/4 25000/25000 [==============================] - 2s 85us/step - loss: 0.0355 - acc: 0.9585 Epoch 3/4 25000/25000 [==============================] - 2s 85us/step - loss: 0.0326 - acc: 0.9641 Epoch 4/4 25000/25000 [==============================] - 2s 85us/step - loss: 0.0289 - acc: 0.9691 25000/25000 [==============================] - 1s 31us/step  results_1L_32u_fit2  [0.09572549975037575, 0.8724]  mod_1L_32u.predict(x_test)  array([[0.12358373], [0.99981403], [0.8463329 ], ..., [0.2399565 ], [0.05467173], [0.76879334]], dtype=float32)   We can see a moderate, perhaps negative impact on model performance. Let\u0026rsquo;s keep tweaking. We will now try an alternative to relu, or \u0026ldquo;rectified linear unit\u0026rdquo; activation.  mod_1L_32u_tanh = models.Sequential() mod_1L_32u_tanh.add(layers.Dense(32, activation='tanh', input_shape=(10000, ))) mod_1L_32u_tanh.add(layers.Dense(1, activation='sigmoid'))  mod_1L_32u_tanh.compile(optimizer='rmsprop', #return compiler to original form loss='binary_crossentropy', metrics=['accuracy'])  mod_1L_32u_tanh.fit(x_train, y_train, epochs=4, batch_size=512) results_1L_32u_fit3 = mod_1L_32u_tanh.evaluate(x_test, y_test)  Epoch 1/4 25000/25000 [==============================] - 2s 84us/step - loss: 0.1381 - acc: 0.9507 Epoch 2/4 25000/25000 [==============================] - 2s 84us/step - loss: 0.1213 - acc: 0.9587 Epoch 3/4 25000/25000 [==============================] - 2s 84us/step - loss: 0.1087 - acc: 0.9629 Epoch 4/4 25000/25000 [==============================] - 2s 84us/step - loss: 0.0994 - acc: 0.9660 25000/25000 [==============================] - 1s 33us/step  results_1L_32u_fit3  [0.4123829656982422, 0.86648]  Our final model to predict movie reviews in the IMDB database, therefore, is our simplest model: 1 dense hidden layer of 32 units, compiled with a binary cross entropy loss function, and a non-linear (\u0026lsquo;relu\u0026rsquo;, or rectified linear unit) activation function. print('With a final model accuracy of: ' + str(results_1L_32u[1]))  With a final model accuracy of: 0.88496  ","date":1565952204,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565952204,"objectID":"1dfc50ae0287842a3909d7886e70b351","permalink":"/post/imdb-rating-prediction-models-v20/","publishdate":"2019-08-16T11:43:24+01:00","relpermalink":"/post/imdb-rating-prediction-models-v20/","section":"post","summary":"Predicting Movie Ratings in Keras - Model Tuning:  We trained a neural network with 4 layers on the Keras IMDB data set in order to predict movie ratings from random training data We reached a local minimum of validation loss and model accuracy of 17% and 94%, respectively.\n We posited a change in:\n  1) Number of tensors/layers\n2) Units per tensor\n3) Parameters of model evaluation/activation that may improve our model\u0026rsquo;s predictive power.","tags":[],"title":"Imdb Rating Prediction Models V20","type":"post"},{"authors":[],"categories":[],"content":" from keras.datasets import imdb (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)  Using TensorFlow backend. WARNING: Logging before flag parsing goes to stderr. W0714 11:56:20.828102 4610053568 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead. W0714 11:56:20.828986 4610053568 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.AttrValue is deprecated. Please use tf.compat.v1.AttrValue instead. W0714 11:56:20.829607 4610053568 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.COMPILER_VERSION is deprecated. Please use tf.version.COMPILER_VERSION instead. W0714 11:56:20.830371 4610053568 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.CXX11_ABI_FLAG is deprecated. Please use tf.sysconfig.CXX11_ABI_FLAG instead. W0714 11:56:20.831021 4610053568 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.ConditionalAccumulator is deprecated. Please use tf.compat.v1.ConditionalAccumulator instead.  #inspect data: train_data[0]  [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 5952, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]  train_labels[0]  1  max([max(sequence) for sequence in train_data])  9999  #we need to vectorize our data: import numpy as np def vect_seq(sequences, dimension=10000): # Create an all-zero matrix of shape (len(sequences), dimension) results = np.zeros((len(sequences), dimension)) for i, sequence in enumerate(sequences): results[i, sequence] = 1. # set specific indices of results[i] to 1s return results  # Our vectorized training data x_train = vect_seq(train_data) # Our vectorized test data x_test = vect_seq(test_data)  x_train[0]  array([0., 1., 1., ..., 0., 0., 0.])  y_train = np.asarray(train_labels).astype('float32') y_test = np.asarray(test_labels).astype('float32')  from keras import models from keras import layers model = models.Sequential() model.add(layers.Dense(16, activation='relu', input_shape=(10000,))) model.add(layers.Dense(16, activation='relu')) model.add(layers.Dense(1, activation='sigmoid'))  #compile the data #model.compile(optimizer='rmsprop', #loss='binary_crossentropy', #metrics=['accuracy'])  from keras import optimizers model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['accuracy'])  Compiling our model (with custom parameters): from keras import losses from keras import metrics model.compile(optimizer=optimizers.RMSprop(lr=0.001), loss=losses.binary_crossentropy, metrics=[metrics.binary_accuracy])  x_val = x_train[:10000] partial_x_train = x_train[10000:] y_val = y_train[:10000] partial_y_train = y_train[10000:]  history = model.fit(partial_x_train, partial_y_train, epochs=20, batch_size=512, validation_data=(x_val, y_val))  Train on 15000 samples, validate on 10000 samples Epoch 1/20 15000/15000 [==============================] - 3s 215us/step - loss: 0.5344 - binary_accuracy: 0.7747 - val_loss: 0.3907 - val_binary_accuracy: 0.8705 Epoch 2/20 15000/15000 [==============================] - 2s 126us/step - loss: 0.3168 - binary_accuracy: 0.8983 - val_loss: 0.3264 - val_binary_accuracy: 0.8717 Epoch 3/20 15000/15000 [==============================] - 2s 120us/step - loss: 0.2334 - binary_accuracy: 0.9249 - val_loss: 0.2858 - val_binary_accuracy: 0.8875 Epoch 4/20 15000/15000 [==============================] - 2s 119us/step - loss: 0.1832 - binary_accuracy: 0.9401 - val_loss: 0.2746 - val_binary_accuracy: 0.8903 Epoch 5/20 15000/15000 [==============================] - 2s 120us/step - loss: 0.1518 - binary_accuracy: 0.9517 - val_loss: 0.3000 - val_binary_accuracy: 0.8805 Epoch 6/20 15000/15000 [==============================] - 2s 119us/step - loss: 0.1223 - binary_accuracy: 0.9635 - val_loss: 0.2894 - val_binary_accuracy: 0.8855 Epoch 7/20 15000/15000 [==============================] - 2s 120us/step - loss: 0.1069 - binary_accuracy: 0.9656 - val_loss: 0.3041 - val_binary_accuracy: 0.8848 Epoch 8/20 15000/15000 [==============================] - 2s 119us/step - loss: 0.0857 - binary_accuracy: 0.9753 - val_loss: 0.3232 - val_binary_accuracy: 0.8788 Epoch 9/20 15000/15000 [==============================] - 2s 119us/step - loss: 0.0693 - binary_accuracy: 0.9827 - val_loss: 0.3418 - val_binary_accuracy: 0.8801 Epoch 10/20 15000/15000 [==============================] - 2s 119us/step - loss: 0.0594 - binary_accuracy: 0.9847 - val_loss: 0.3804 - val_binary_accuracy: 0.8773 Epoch 11/20 15000/15000 [==============================] - 2s 121us/step - loss: 0.0461 - binary_accuracy: 0.9899 - val_loss: 0.3959 - val_binary_accuracy: 0.8739 Epoch 12/20 15000/15000 [==============================] - 2s 119us/step - loss: 0.0375 - binary_accuracy: 0.9927 - val_loss: 0.4292 - val_binary_accuracy: 0.8779 Epoch 13/20 15000/15000 [==============================] - 2s 118us/step - loss: 0.0295 - binary_accuracy: 0.9947 - val_loss: 0.4603 - val_binary_accuracy: 0.8683 Epoch 14/20 15000/15000 [==============================] - 2s 123us/step - loss: 0.0247 - binary_accuracy: 0.9956 - val_loss: 0.4999 - val_binary_accuracy: 0.8745 Epoch 15/20 15000/15000 [==============================] - 2s 118us/step - loss: 0.0210 - binary_accuracy: 0.9955 - val_loss: 0.5153 - val_binary_accuracy: 0.8703 Epoch 16/20 15000/15000 [==============================] - 2s 117us/step - loss: 0.0118 - binary_accuracy: 0.9991 - val_loss: 0.5965 - val_binary_accuracy: 0.8665 Epoch 17/20 15000/15000 [==============================] - 2s 118us/step - loss: 0.0124 - binary_accuracy: 0.9984 - val_loss: 0.5746 - val_binary_accuracy: 0.8690 Epoch 18/20 15000/15000 [==============================] - 2s 121us/step - loss: 0.0077 - binary_accuracy: 0.9994 - val_loss: 0.7067 - val_binary_accuracy: 0.8470 Epoch 19/20 15000/15000 [==============================] - 2s 119us/step - loss: 0.0054 - binary_accuracy: 0.9998 - val_loss: 0.6341 - val_binary_accuracy: 0.8669 Epoch 20/20 15000/15000 [==============================] - 2s 118us/step - loss: 0.0072 - binary_accuracy: 0.9987 - val_loss: 0.6585 - val_binary_accuracy: 0.8656  history_dict = history.history history_dict.keys()  dict_keys(['val_loss', 'val_binary_accuracy', 'loss', 'binary_accuracy'])  Training and Validation loss: import matplotlib.pyplot as plt acc = history.history['binary_accuracy'] val_acc = history.history['val_binary_accuracy'] loss = history.history['loss'] val_loss = history.history['val_loss'] epochs = range(1, len(acc) + 1) # \u0026quot;bo\u0026quot; is for \u0026quot;blue dot\u0026quot; plt.plot(epochs, loss, 'bo', label='Training loss') # b is for \u0026quot;solid blue line\u0026quot; plt.plot(epochs, val_loss, 'b', label='Validation loss') plt.title('Training and validation loss') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend() plt.show()  \u0026lt;Figure size 640x480 with 1 Axes\u0026gt;  Training and Validation Accuracy: plt.clf() # clears prior figure acc_values = history_dict['binary_accuracy'] val_acc_values = history_dict['val_binary_accuracy'] plt.plot(epochs, acc, 'bo', label='Training acc') plt.plot(epochs, val_acc, 'b', label='Validation acc') plt.title('Training and validation accuracy') plt.xlabel('Epochs') plt.ylabel('Loss') plt.legend() plt.show()  This comparison is a primary exampe of overfitting. Look at the difference of how metrics of accuracy and loss peak around the fourth epoch.\nAn appropriate response could be to simply stop our fitting of training data at 4 epochs. Let\u0026rsquo;s do this now:\n#train new model from scratch model = models.Sequential() model.add(layers.Dense(16, activation='relu', input_shape=(10000,))) model.add(layers.Dense(16, activation='relu')) model.add(layers.Dense(1, activation='sigmoid')) model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy']) model.fit(x_train, y_train, epochs=4, batch_size=512) results = model.evaluate(x_test, y_test)  Epoch 1/4 25000/25000 [==============================] - 2s 82us/step - loss: 0.4749 - acc: 0.8217 Epoch 2/4 25000/25000 [==============================] - 2s 74us/step - loss: 0.2666 - acc: 0.9094 Epoch 3/4 25000/25000 [==============================] - 2s 74us/step - loss: 0.1985 - acc: 0.9294 Epoch 4/4 25000/25000 [==============================] - 2s 74us/step - loss: 0.1680 - acc: 0.9405 25000/25000 [==============================] - 2s 77us/step  results  [0.32515703952789304, 0.87256]  Now, we would like to implement our model using new data:  What are the likelihoods of generating positive reviews? Use \u0026lsquo;predict\u0026rsquo; function from Keras:  model.predict(x_test)  array([[0.13455817], [0.99970734], [0.2682383 ], ..., [0.07024318], [0.0425705 ], [0.47154084]], dtype=float32)  This shows a wide range of prediction accuracy- from 0.13 to upwards of 0.99.\nWhat else can we do to improve?\nNext steps: 1) We could use a different number of hidden layers (in this example we trained 2 layers) 2) Our layers could contain a different number of \u0026lsquo;units\u0026rsquo; 3) Our loss function could be re-tooled- using \u0026lsquo;mse\u0026rsquo; rather than binary cross entropy 4) We could implement a different method of lahyer activation. We have been using \u0026lsquo;relu\u0026rsquo; but what of other methods, such as the common \u0026lsquo;tanh\u0026rsquo; activation function?\nLet\u0026rsquo;s be good scientists and experiment with all (holding other variables equal), with the explicit goal of improving our model accuracy and efficiency.\nNOTE: MODEL TUNING TO BE ACCOMPLISHED IN VOLUME 2 OF THIS NOTEBOOK. ","date":1565952187,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565952187,"objectID":"984b77c6ce0816faf0a8f111424142a4","permalink":"/post/imdb-rating-prediction-v1/","publishdate":"2019-08-16T11:43:07+01:00","relpermalink":"/post/imdb-rating-prediction-v1/","section":"post","summary":"from keras.datasets import imdb (train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)  Using TensorFlow backend. WARNING: Logging before flag parsing goes to stderr. W0714 11:56:20.828102 4610053568 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead. W0714 11:56:20.828986 4610053568 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.AttrValue is deprecated. Please use tf.compat.v1.AttrValue instead. W0714 11:56:20.829607 4610053568 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.COMPILER_VERSION is deprecated. Please use tf.version.COMPILER_VERSION instead. W0714 11:56:20.830371 4610053568 deprecation_wrapper.","tags":[],"title":"Imdb Rating Prediction V1","type":"post"},{"authors":null,"categories":null,"content":"Today, I kicked off my first day of class and independent study through MIT\u0026rsquo;s open courseware, \u0026ldquo;Introduction to Computer Science and Programming using Python.\u0026rdquo; I have done some modest prep work to this end, having already podcasted the first five lectures on long rides to the gym, but with how I learn maths and logic, this kind of familiarity is almost necessary.\nWe are covering the philosophical underpinnings of computer science and thinking algorithmically. Thinking of an algorithm as a recipe that is repeated has to the the best analogy I have heard to date.\nNext, we performed an exercise to break down the algorithm embedded in the \u0026ldquo;square\u0026rdquo; function in mathematics. It posits that if you have some guess, \u0026ldquo;g,\u0026rdquo; and multiply g by itself to get some result, \u0026ldquo;x,\u0026rdquo; you can repeatedly walk through a process of guessing, adding the guess to the guess divided by our x (so g + x/g) then divide that by 2 (average it) and come increasingly close to the desired outcome. Don\u0026rsquo;t believe me? Try it with the assumption that we do not know the square root of 16. Maybe we start with 3. 3 times 3 is 9\u0026hellip; which ain\u0026rsquo;t 16. Keep going! 3.5? Almost there, but this comes out to 12.25. Again, not 16. As we see our guesses converge nd get closer to 16 we can STOP! Bam. That\u0026rsquo;s an algorithm that we can even represent on our computers. My next task will be to try and programmatically define my own square root function (hopefully by tomorrow!).\nA bit of meta: I have also arrived to a morning schedule that is productive and mindful enough for me to be sustainable. It starts by 1) waking up before 6:30, 2)either jumping straight into a coding challenge (after sufficient caffeine restoration) or workout, 3) a few laps around Audubon park on bike while working on the coding challenges in my head, 4) at least 10 minutes sitting meditation, and 5) an ice cold shower (it\u0026rsquo;s hot here y\u0026rsquo;all, even by 8 am). I like this approach for several reasons: First, the park is quiet and I am the most mentally sharp in the mornings. Also, even if I tank the rest of my day looking at reddit or dream jobs, I can feel good knowing my morning packed alot of devoted deep work in.\nHere goes nothin\u0026rsquo;\n","date":1559692800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1559692800,"objectID":"81de7d554a551b98a347335c2bcbf808","permalink":"/post/2019-06-05-walker-goes-back-to-school-again/","publishdate":"2019-06-05T00:00:00Z","relpermalink":"/post/2019-06-05-walker-goes-back-to-school-again/","section":"post","summary":"Today, I kicked off my first day of class and independent study through MIT\u0026rsquo;s open courseware, \u0026ldquo;Introduction to Computer Science and Programming using Python.\u0026rdquo; I have done some modest prep work to this end, having already podcasted the first five lectures on long rides to the gym, but with how I learn maths and logic, this kind of familiarity is almost necessary.\nWe are covering the philosophical underpinnings of computer science and thinking algorithmically.","tags":null,"title":"J. Walker Blackston, aspiring data scientist, goes back to school (again)","type":"post"},{"authors":null,"categories":null,"content":"It only took a few hours before I realized the foolishness of my mistake. As I mentioned last, I forgot to save my successful code to run a D\u0026amp;D dice simulator that would print advantage or disadvantage rolls. I am now refactoring it with a tutorial I remember from deep in the ether of other simple python tutorials. I will highlight where this differs from my function. This function brought to you by O\u0026rsquo;Reilly Python cookbooks:\nimport random def die(num, sides): #num equals number of die, sides equal sides of each return reduce(lambda x, y, s=sides: x + random.randrange(s), range(num+1)) + num  There are a couple of things to note. 1) This code is almost embarassingly simple and allows me to see how much work I have left to truly understand writing good code, and 2) it makes use of some functions I simply do not have stored in my mental roledex for use as needed.\nMy first attempt had me thinking I would need to initialize the length of the dice, forgetting that the random module would allow a dice to roll and choose a random number from a given range. Then I realized that my input and main argument for the dice() function were the same, causing roll to be undefined in the global space but defined locally as a simple input() string. I also first thought that creating a disadvantaged roll and advantaged roll would require separate functions, now I see that it can be included as one succinct if-then statement. Adding this to the function above, we get:\nimport random import numpy as np from functools import reduce def dice(num, sides): def accumulate(x, y, s=sides): return x + random.randrange(s) return reduce(accumulate, range(num+1)) + num print('How many dice are you rolling?') valid_die = ['1', '2'] num = [] die = input() if die in valid_die: num = int(die) else: print('Please enter a valid number of die') print('How many sides per die?') valid_sides = ['4', '6', '8', '20'] sides = [] side_per = input() if side_per in valid_sides: sides = int(side_per) else: print('Please enter valid number of sides per die')  This provides a much more definitive program, although I still think it could be simplified and refactored because the function itself only returns one number from a dice roll.\n","date":1558915200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558915200,"objectID":"7f1d33f3d9f6bd8ddde0b5ff24d457ea","permalink":"/post/2019-05-27-walker-gets-humbled/","publishdate":"2019-05-27T00:00:00Z","relpermalink":"/post/2019-05-27-walker-gets-humbled/","section":"post","summary":"It only took a few hours before I realized the foolishness of my mistake. As I mentioned last, I forgot to save my successful code to run a D\u0026amp;D dice simulator that would print advantage or disadvantage rolls. I am now refactoring it with a tutorial I remember from deep in the ether of other simple python tutorials. I will highlight where this differs from my function. This function brought to you by O\u0026rsquo;Reilly Python cookbooks:","tags":null,"title":"J. Walker Blackston, python noob, writes first own function that works, immediately creates a mistake","type":"post"},{"authors":null,"categories":null,"content":"Today I played roughly 4+ hours of what became my first Dungeons and Dragons campaign. High school me rolls his eyes, but he can shove it because I am both smarter and stronger than he is.\nIn all seriousness, I see the appeal. The genuine joy you experience when hilarious shit happens to you or your buddies ios rarely duplicated. I don\u0026rsquo;t expect to be swapping my gym membership for a D4 games budget any time soon (a local D\u0026amp;D hangout), but I\u0026rsquo;ll be damned if I didn\u0026rsquo;t have a blast and will continue with this campaign occasionally.\nImportantly, this experience brought me an idea for my first \u0026ldquo;personal project\u0026rdquo; to write in python. I realized this when thinking about how traits and turns are structured. To the un-churched, nearly everything in D\u0026amp;D revolves around a series of multi-sided die. A few are 20-sided (or \u0026ldquo;d20\u0026rdquo;), where others are d6 or d4 etc., all with equal probability of rolling. These rolls dictate player traits, attack abilities, and any other special attributes related to your character arc.\nOne condition I\u0026rsquo;d like to play with here is the concept of \u0026ldquo;advantaged\u0026rdquo; or \u0026ldquo;disadvantaged\u0026rdquo; rolls. This simply means that when you roll 2 d20\u0026rsquo;s, you take the lower of the rolls. If you were creating a D\u0026amp;D Python game, how would you simulate these rolls and make a decision?\n\u0026rdquo;\u0026rsquo; #let r_1 and r_2 be the set of possible rolls r_1 = list(range(1, 20, 1) r_2 = list(range(1, 20, 1) #create a function that will take in the move of both, draw randomly with equal probability for all numbers, and make a simple decision to give advantage:\nimport random import numpy as np def die():\nprint('Please roll: ') roll = input() if roll != 'roll': print('Please type \u0026quot;roll\u0026quot; to roll die') else: roll_one = list(range(1, 21)) roll_two = list(range(1, 21)) if roll_one \u0026gt; roll_two: print('You rolled a ' + str(roll_one) + ' and ' + str(roll_two) + '. ' 'Advantage: ' + str(roll_one)) roll = roll_one return roll_one elif roll_one \u0026lt; roll_two: print('Disadvantage: ' + str(roll_two)) roll = roll_two return roll_two  \u0026rdquo;\u0026rsquo; I got the first iteration of this to run! Holy cow that felt cool. But then I got cheeky and decided to refactor. Due to some pointers from a professional, I decided to simplify and clean it up, which represents the code you see above. This has taught me several lessons: 1) always save checkpoints of successful code, and 2) if it ain\u0026rsquo;t broke, don\u0026rsquo;t fix it. So now I will be refactoring this code to make it run again.\nHere goes nothin\u0026rsquo;\n","date":1558828800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558828800,"objectID":"c3dc1e01adc0aa61328f661d6d39fc9a","permalink":"/post/2019-05-26-walker-plays-dd/","publishdate":"2019-05-26T00:00:00Z","relpermalink":"/post/2019-05-26-walker-plays-dd/","section":"post","summary":"Today I played roughly 4+ hours of what became my first Dungeons and Dragons campaign. High school me rolls his eyes, but he can shove it because I am both smarter and stronger than he is.\nIn all seriousness, I see the appeal. The genuine joy you experience when hilarious shit happens to you or your buddies ios rarely duplicated. I don\u0026rsquo;t expect to be swapping my gym membership for a D4 games budget any time soon (a local D\u0026amp;D hangout), but I\u0026rsquo;ll be damned if I didn\u0026rsquo;t have a blast and will continue with this campaign occasionally.","tags":null,"title":"J. Walker Blackston, python and board game novice, tries Dungeons and Dragons","type":"post"},{"authors":null,"categories":null,"content":"Last time we spoke, I was trying to grasp how functions worked, failing miserably to get a web scraper up and running, and suffering from the general malaise of an over-educated mid-20\u0026rsquo;s male who stares at his phone too much.\nBeyond the more existential concerns, I have decided (with the help of my lovely girlfriend) to design a mini summer course of self-study. Thanks to MIT\u0026rsquo;s open courseware, I will take my series of intros to computation and computer science (with a focus in python), algorithms, and potentially some dedicated software developmennt coursework. While many seem to disagree about the best approach for learning something like programming, I will go with the prevailing wisdom that simpler is better in this regard. Lectures, exams, paper, pencil, and whiteboards- the hard and boring way. I will limit my use of a browser as that can be distracting. This may not directly feed into the didactic goals of my PhD, but it will be important that I sharpen these skills for the eventual job search in data science (if academic epidemiology doesn\u0026rsquo;t take shape).\nBy the end of June, I hope to have completed 6.0001 and 6.0002, with the goal of more concrete theoretical understanding of how programs work.\nHere goes nothin\u0026rsquo;\n","date":1558742400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558742400,"objectID":"ee47e9fa15da83c0d0634a2dfc7746bd","permalink":"/post/2019-05-25-walker-designs-curriculum/","publishdate":"2019-05-25T00:00:00Z","relpermalink":"/post/2019-05-25-walker-designs-curriculum/","section":"post","summary":"Last time we spoke, I was trying to grasp how functions worked, failing miserably to get a web scraper up and running, and suffering from the general malaise of an over-educated mid-20\u0026rsquo;s male who stares at his phone too much.\nBeyond the more existential concerns, I have decided (with the help of my lovely girlfriend) to design a mini summer course of self-study. Thanks to MIT\u0026rsquo;s open courseware, I will take my series of intros to computation and computer science (with a focus in python), algorithms, and potentially some dedicated software developmennt coursework.","tags":null,"title":"J. Walker Blackston, aspiring data scientist, Designs summer curriculum","type":"post"},{"authors":null,"categories":null,"content":"Well, this morning I worked through a few guided phases of DataQuest\u0026rsquo;s Python For Data Science project which seeks to analyze app data from the Google Play and iOS Apple Stores.\nThe project, while outwardly simple, contains many steps that are common to data science. To successfully import and clean our data, we needed to create some personalized functions that can handle our specific needs. In this case, it was the need to identify non-English characters (based on ASCII indexing) and scrub duplicates from our data at the same time. We made some really handy functions for this, the first being a data exploration tool that will provide spaces between some highlighted rows and count/print the number of columns and rows in the set:\ndef explore_data(dataset, start, end, rows_cols = False): data_slice = dataset[start:end] for row in data_slice: print(row) print('\\n') if rows_cols: print('Number of rows:', len(dataset)) print('Number of columns:', len(dataset[0]))  Then we wrote a short program that will detect duplicates in data. I know there are likely better functions out there for this, but it was cool to create my own and see what the computer reads through to do so:\ndup_rows = [] #initialize empty lists unique_rows = [] for dat in data: name = dat[0] if name in unique_rows: dup_rows.append(name) else: unique_row.append(name)  Which will be updated to include an \u0026lsquo;is_english\u0026rsquo; function to detect non-ASCII characters in a string:\ndef is_english(string): non_am = 0 for char in string: if ord(character) \u0026gt; 127: non_am += 1 if non_am \u0026gt; 3: return False else: return True  Which is also just super neat because it uses the built-in function, \u0026lsquo;ord()\u0026rsquo; to tell us the index of characters passed into the string parameter, iterate over however many characters are in the string, and spit out a counter of non-ASCII values. In general, if a string contains more than 3 non-ASCII it\u0026rsquo;s apparently considered non-English. Now I know!\nAnyhow, this was super hard to do without peaking, and I think I know why. I still cannot intuit the way that functions can and should handle variables you pass in. For example, when the dup_rows.append(name) command is specified, I wanted to write it initially with the parameter \u0026lsquo;dat\u0026rsquo; because that\u0026rsquo;s the iterator. But the purpose of the function is to take the iterator to craft a new variable that was originally empty. This will just take practice I guess :/\nHere goes nothin\u0026rsquo;\n","date":1558569600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558569600,"objectID":"0fc022181b09d5f0f8a6a35f7cb76a06","permalink":"/post/2019-05-23-walker-blackston-tries-more-functions/","publishdate":"2019-05-23T00:00:00Z","relpermalink":"/post/2019-05-23-walker-blackston-tries-more-functions/","section":"post","summary":"Well, this morning I worked through a few guided phases of DataQuest\u0026rsquo;s Python For Data Science project which seeks to analyze app data from the Google Play and iOS Apple Stores.\nThe project, while outwardly simple, contains many steps that are common to data science. To successfully import and clean our data, we needed to create some personalized functions that can handle our specific needs. In this case, it was the need to identify non-English characters (based on ASCII indexing) and scrub duplicates from our data at the same time.","tags":null,"title":"J. Walker Blackston, aspring data scientist, Struggles with functions for cleaning data","type":"post"},{"authors":null,"categories":null,"content":"As I am still launching this website and learning what it does (and more often, does not do) I am reminded of the vast interconnectedness of programming languages. No language can truly be learned in isolation\u0026hellip;\nTake this blog for example. In order for me to demonstrate what I learn, I need to understand how markdown will process my commands. I have to learn its basics before I can demonstrate myself learning the basics of python with in-line code. Confused yet?\nAt any rate, this post is really a procrastination technique for my next chapter of \u0026ldquo;Automate the Boring Stuff\u0026rdquo; on Lists.\nHere goes nothin\u0026rsquo;\nPost mortem: I just worked on python functions that can return desired parameters 1) to the exclusion of the other parameters, accomplished through an \u0026lsquo;if-else\u0026rsquo; code block, and 2) that return statements can feature ALL possible parameters of the function. I also learned about the importance of setting defaults to your arguments when a function will be used repeatedly for the same data or object.\nPost post mortem: It turns out its very difficult to add google analytics and comments into your blog, so that will be a learning lesson. That also caused the delay in this post (the posts directory wouldn\u0026rsquo;t commit because of faulty code).\n","date":1558396800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558396800,"objectID":"f0790754c664885e73a4bb8cf62726a4","permalink":"/post/2019-05-21-python_lessons/","publishdate":"2019-05-21T00:00:00Z","relpermalink":"/post/2019-05-21-python_lessons/","section":"post","summary":"As I am still launching this website and learning what it does (and more often, does not do) I am reminded of the vast interconnectedness of programming languages. No language can truly be learned in isolation\u0026hellip;\nTake this blog for example. In order for me to demonstrate what I learn, I need to understand how markdown will process my commands. I have to learn its basics before I can demonstrate myself learning the basics of python with in-line code.","tags":null,"title":"J. Walker Blackston, utter programming novice, Starts learning python for data science","type":"post"},{"authors":null,"categories":null,"content":"Well. Finally got around to putting this old website together. Neat thing about it - powered by Jekyll and I can use Markdown to author my posts. It actually is a lot easier than I thought it was going to be.\nHere goes nothin\u0026rsquo;!\n","date":1558224000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558224000,"objectID":"3637a7daee6e8238e889505c0e62ecb4","permalink":"/post/2019-05-19-walker-blackston-site-launched/","publishdate":"2019-05-19T00:00:00Z","relpermalink":"/post/2019-05-19-walker-blackston-site-launched/","section":"post","summary":"Well. Finally got around to putting this old website together. Neat thing about it - powered by Jekyll and I can use Markdown to author my posts. It actually is a lot easier than I thought it was going to be.\nHere goes nothin\u0026rsquo;!","tags":null,"title":"J. Walker Blackston, utter programming novice, launches website","type":"post"},{"authors":["Walker Blackston"],"categories":null,"content":" Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":" Welcome to Slides Academic\nFeatures  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides  Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E  Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = \\;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \nA fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears  Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view   Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links   night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links  Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/img/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"8f66d660a9a2edc2d08e68cc30f701f7","permalink":"/project/internal-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/internal-project/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Internal Project","type":"project"},{"authors":["Walker Blackston","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Walker Blackston","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software.    Click the Slides button above to demo Academic\u0026rsquo;s Markdown slides feature.   Supplementary notes can be added here, including code and math.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example conference paper","type":"publication"}]