<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Walker Blackston</title>
    <link>/post/</link>
    <description>Recent content in Posts on Walker Blackston</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator>
    <language>en-us</language>
    <copyright>&amp;copy; Walker Blackston 2019</copyright>
    <lastBuildDate>Fri, 16 Aug 2019 11:44:46 +0100</lastBuildDate>
    
	    <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Topcoffee</title>
      <link>/post/topcoffee/</link>
      <pubDate>Fri, 16 Aug 2019 11:44:46 +0100</pubDate>
      
      <guid>/post/topcoffee/</guid>
      <description>

&lt;h1 id=&#34;top-coffee-near-you&#34;&gt;Top Coffee Near You:&lt;/h1&gt;

&lt;p&gt;Too often, we are confronted with the paradox of choice for delicious things. Do I go here for buttery croissants but crappy coffee? Do I hit up this swanky joint with dope espresso, but at the expense of nay any food items and perhaps a small percentage of my student loan?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;This program can help.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Every time I go to a new place, I sift all outlets for the best coffee (reddit, yelp, carrier pigeon, etc.) but this can be a lengthy and draining process. Why not have a program that you can type in your location and receive just the top three shops, according to average yelp review?&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Let&amp;rsquo;s give it a shot&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#import relevant packages

from bs4 import BeautifulSoup 
import requests, bs4, csv
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;First, we&amp;rsquo;d like to specify your location and store it for our parser:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;location = input(&amp;quot;Where y&#39;at?: &amp;quot;)
soup = requests.get(&amp;quot;https://www.yelp.com/search?find_desc=Coffee+Shops&amp;amp;find_loc=&amp;quot; + location).text
print(&#39;To confirm, you want to look at &#39; + location + &amp;quot;?&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Where y&#39;at?: New Orleans
To confirm, you want to look at New Orleans?
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;conf = input(&amp;quot;Confirm Y or N: &amp;quot;)

if conf == &#39;N&#39;or conf == &#39;n&#39;:
    print(&amp;quot;Let&#39;s try this again shall we?&amp;quot;)
    location = input(&amp;quot;New Location: &amp;quot;)

elif conf == &#39;Y&#39; or conf == &#39;y&#39;:
    print(&#39;Good coffee incoming!&#39;)

else:
    print(&#39;Confirm with a Yes or No only, you wild person.&#39;)

#input a Y or N below:
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Confirm Y or N: Y
Good coffee incoming!
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Solid. Now that you have picked and confirmed your location, let&amp;rsquo;s get that beautiful soup brewing. I am still learning this functionality in Python, so it may time some time and refactoring to get everything right, but for now, I will import other uses and adapt it.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results = soup.find(&#39;h3&#39;, string=&#39;All Results&#39;).find_parent(&#39;li&#39;).find_next_siblings(&#39;li&#39;)
len(results) #check the length of results
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;------------------------------------------------------------------------

TypeError                              Traceback (most recent call last)

&amp;lt;ipython-input-13-985d6118bc72&amp;gt; in &amp;lt;module&amp;gt;
----&amp;gt; 1 results = soup.find(&#39;h3&#39;, string=&#39;All Results&#39;).find_parent(&#39;li&#39;).find_next_siblings(&#39;li&#39;)
      2 len(results) #check the length of results


TypeError: find() takes no keyword arguments
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This soup function allows us to go through the argument type &amp;lsquo;h3&amp;rsquo; of an HTML, find the string &amp;lsquo;All Results&amp;rsquo; since we are not interested in sponsored content, then looks upward and across the HTML code trees using &amp;lsquo;.find_parent&amp;rsquo; and &amp;lsquo;.find_next_siblings&amp;rsquo; for other instances of the link tag.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#print(soup.prettify()) #uncomment to see the full elemental print of the site in HTML
tabl_ratings = soup.find(&#39;div&#39;, attrs = {&#39;class&#39;: &#39;lemon--&#39;})

for row in tabl_ratings.findAll(&#39;h3&#39;, attrs = {&#39;class&#39;: &#39;All Results&#39;}):
    coffee = {}
    coffee[&#39;shop name&#39;] = row.a[&#39;target name&#39;] 
    coffee[&#39;rating&#39;] = row.div[&#39;aria-label&#39;]
    coffee.append(coffee)


    
    
top_3 = {}   # Top 3 places for coffee, stored as a dictionary
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Gunviolenceneworleans</title>
      <link>/post/gunviolenceneworleans/</link>
      <pubDate>Fri, 16 Aug 2019 11:44:39 +0100</pubDate>
      
      <guid>/post/gunviolenceneworleans/</guid>
      <description>

&lt;h1 id=&#34;the-state-of-gun-violence-in-new-orleans&#34;&gt;The State of Gun Violence in New Orleans&lt;/h1&gt;

&lt;p&gt;&lt;strong&gt;New Orleans, Louisiana&lt;/strong&gt;, my dearest hometown, has a complicated relationship with the world. Many come here to party or test their cajun linguistic ability. Maybe you come for the food? Regardless, many are also aware of its issues with public safety. I am looking to test the hypothesis that New Orleans is or is not safe, with open sourced data on gun violence as a proxy for this notion of safety (the Gun Violence Archive or GVA: &lt;a href=&#34;https://www.gunviolencearchive.org&#34; target=&#34;_blank&#34;&gt;https://www.gunviolencearchive.org&lt;/a&gt;). Data range from January 21, 2013 to March 30, 2018.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;All thoughts are my own, and these data are open. Please feel free to question, test, replicate and challenge my analytic approach with your own work. As I am very much in the learning phase, please excuse any over-commenting and mistakes and/or clunky code. Thanks, JWB&lt;/em&gt;&lt;/p&gt;

&lt;h3 id=&#34;step-0-packages-and-importing-data&#34;&gt;Step 0: Packages and Importing Data&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#Importing all packages and data:
%matplotlib inline
import numpy as np
import pandas as pd
import matplotlib 
import matplotlib.pyplot as plt
import seaborn as sb
from sklearn.preprocessing import MinMaxScaler

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;file = &amp;quot;/Users/walker/Desktop/Python/Data/stage3.csv&amp;quot;
gv_df = pd.read_csv(file, encoding = &#39;utf-8&#39;)

#print the shape of the data to get observations and variables (rows, columns)
gv_df.shape
gv_df.head()

&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;incident_id&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;state&lt;/th&gt;
      &lt;th&gt;city_or_county&lt;/th&gt;
      &lt;th&gt;address&lt;/th&gt;
      &lt;th&gt;n_killed&lt;/th&gt;
      &lt;th&gt;n_injured&lt;/th&gt;
      &lt;th&gt;incident_url&lt;/th&gt;
      &lt;th&gt;source_url&lt;/th&gt;
      &lt;th&gt;incident_url_fields_missing&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;participant_age&lt;/th&gt;
      &lt;th&gt;participant_age_group&lt;/th&gt;
      &lt;th&gt;participant_gender&lt;/th&gt;
      &lt;th&gt;participant_name&lt;/th&gt;
      &lt;th&gt;participant_relationship&lt;/th&gt;
      &lt;th&gt;participant_status&lt;/th&gt;
      &lt;th&gt;participant_type&lt;/th&gt;
      &lt;th&gt;sources&lt;/th&gt;
      &lt;th&gt;state_house_district&lt;/th&gt;
      &lt;th&gt;state_senate_district&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;461105&lt;/td&gt;
      &lt;td&gt;2013-01-01&lt;/td&gt;
      &lt;td&gt;Pennsylvania&lt;/td&gt;
      &lt;td&gt;Mckeesport&lt;/td&gt;
      &lt;td&gt;1506 Versailles Avenue and Coursin Street&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;http://www.gunviolencearchive.org/incident/461105&lt;/td&gt;
      &lt;td&gt;http://www.post-gazette.com/local/south/2013/0...&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0::20&lt;/td&gt;
      &lt;td&gt;0::Adult 18+||1::Adult 18+||2::Adult 18+||3::A...&lt;/td&gt;
      &lt;td&gt;0::Male||1::Male||3::Male||4::Female&lt;/td&gt;
      &lt;td&gt;0::Julian Sims&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0::Arrested||1::Injured||2::Injured||3::Injure...&lt;/td&gt;
      &lt;td&gt;0::Victim||1::Victim||2::Victim||3::Victim||4:...&lt;/td&gt;
      &lt;td&gt;http://pittsburgh.cbslocal.com/2013/01/01/4-pe...&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;460726&lt;/td&gt;
      &lt;td&gt;2013-01-01&lt;/td&gt;
      &lt;td&gt;California&lt;/td&gt;
      &lt;td&gt;Hawthorne&lt;/td&gt;
      &lt;td&gt;13500 block of Cerise Avenue&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;http://www.gunviolencearchive.org/incident/460726&lt;/td&gt;
      &lt;td&gt;http://www.dailybulletin.com/article/zz/201301...&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0::20&lt;/td&gt;
      &lt;td&gt;0::Adult 18+||1::Adult 18+||2::Adult 18+||3::A...&lt;/td&gt;
      &lt;td&gt;0::Male&lt;/td&gt;
      &lt;td&gt;0::Bernard Gillis&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0::Killed||1::Injured||2::Injured||3::Injured&lt;/td&gt;
      &lt;td&gt;0::Victim||1::Victim||2::Victim||3::Victim||4:...&lt;/td&gt;
      &lt;td&gt;http://losangeles.cbslocal.com/2013/01/01/man-...&lt;/td&gt;
      &lt;td&gt;62.0&lt;/td&gt;
      &lt;td&gt;35.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;478855&lt;/td&gt;
      &lt;td&gt;2013-01-01&lt;/td&gt;
      &lt;td&gt;Ohio&lt;/td&gt;
      &lt;td&gt;Lorain&lt;/td&gt;
      &lt;td&gt;1776 East 28th Street&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;3&lt;/td&gt;
      &lt;td&gt;http://www.gunviolencearchive.org/incident/478855&lt;/td&gt;
      &lt;td&gt;http://chronicle.northcoastnow.com/2013/02/14/...&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0::25||1::31||2::33||3::34||4::33&lt;/td&gt;
      &lt;td&gt;0::Adult 18+||1::Adult 18+||2::Adult 18+||3::A...&lt;/td&gt;
      &lt;td&gt;0::Male||1::Male||2::Male||3::Male||4::Male&lt;/td&gt;
      &lt;td&gt;0::Damien Bell||1::Desmen Noble||2::Herman Sea...&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0::Injured, Unharmed, Arrested||1::Unharmed, A...&lt;/td&gt;
      &lt;td&gt;0::Subject-Suspect||1::Subject-Suspect||2::Vic...&lt;/td&gt;
      &lt;td&gt;http://www.morningjournal.com/general-news/201...&lt;/td&gt;
      &lt;td&gt;56.0&lt;/td&gt;
      &lt;td&gt;13.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;478925&lt;/td&gt;
      &lt;td&gt;2013-01-05&lt;/td&gt;
      &lt;td&gt;Colorado&lt;/td&gt;
      &lt;td&gt;Aurora&lt;/td&gt;
      &lt;td&gt;16000 block of East Ithaca Place&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;http://www.gunviolencearchive.org/incident/478925&lt;/td&gt;
      &lt;td&gt;http://www.dailydemocrat.com/20130106/aurora-s...&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0::29||1::33||2::56||3::33&lt;/td&gt;
      &lt;td&gt;0::Adult 18+||1::Adult 18+||2::Adult 18+||3::A...&lt;/td&gt;
      &lt;td&gt;0::Female||1::Male||2::Male||3::Male&lt;/td&gt;
      &lt;td&gt;0::Stacie Philbrook||1::Christopher Ratliffe||...&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0::Killed||1::Killed||2::Killed||3::Killed&lt;/td&gt;
      &lt;td&gt;0::Victim||1::Victim||2::Victim||3::Subject-Su...&lt;/td&gt;
      &lt;td&gt;http://denver.cbslocal.com/2013/01/06/officer-...&lt;/td&gt;
      &lt;td&gt;40.0&lt;/td&gt;
      &lt;td&gt;28.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;478959&lt;/td&gt;
      &lt;td&gt;2013-01-07&lt;/td&gt;
      &lt;td&gt;North Carolina&lt;/td&gt;
      &lt;td&gt;Greensboro&lt;/td&gt;
      &lt;td&gt;307 Mourning Dove Terrace&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;2&lt;/td&gt;
      &lt;td&gt;http://www.gunviolencearchive.org/incident/478959&lt;/td&gt;
      &lt;td&gt;http://www.journalnow.com/news/local/article_d...&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;0::18||1::46||2::14||3::47&lt;/td&gt;
      &lt;td&gt;0::Adult 18+||1::Adult 18+||2::Teen 12-17||3::...&lt;/td&gt;
      &lt;td&gt;0::Female||1::Male||2::Male||3::Female&lt;/td&gt;
      &lt;td&gt;0::Danielle Imani Jameison||1::Maurice Eugene ...&lt;/td&gt;
      &lt;td&gt;3::Family&lt;/td&gt;
      &lt;td&gt;0::Injured||1::Injured||2::Killed||3::Killed&lt;/td&gt;
      &lt;td&gt;0::Victim||1::Victim||2::Victim||3::Subject-Su...&lt;/td&gt;
      &lt;td&gt;http://myfox8.com/2013/01/08/update-mother-sho...&lt;/td&gt;
      &lt;td&gt;62.0&lt;/td&gt;
      &lt;td&gt;27.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 29 columns&lt;/p&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;Looks like our data will include many US cities- not just New Orleans. Also, looks like it contains identifiable characteristics, such as name, that will be scrubbed from further analysis due to irrelevancy.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;step-1-filter-and-clean-data&#34;&gt;Step 1: Filter and Clean Data&lt;/h3&gt;

&lt;p&gt;Now that we have our raw data from a .csv, we want to shape it to answer our question specific to New Orleans. This will require finding which column provides the City of the report, then filtering our data by this variable (in this case, &amp;lsquo;city_or_county&amp;rsquo;). I will attempt to filter by the condition of &amp;lsquo;city_or_county&amp;rsquo; being equal to New Orleans.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;aside: As I mentioned, I am still learning how to filter data sets in Python. I performed the filtering in R and re-imported a .csv here. The below code was the beginning of my work on this after several attempts.&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#is_nola = gv_nola_df[&#39;city_or_county&#39;] == &#39;New Orleans&#39;

#print(is_nola)

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#gv_nola_df = gv_nola_df[is_nola]

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;new_file = &amp;quot;/Users/walker/Desktop/gv_NO.csv&amp;quot;
gv_no = pd.read_csv(new_file, encoding = &#39;utf-8&#39;)

gv_no.head()
&lt;/code&gt;&lt;/pre&gt;

&lt;div&gt;
&lt;style scoped&gt;
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
&lt;/style&gt;
&lt;table border=&#34;1&#34; class=&#34;dataframe&#34;&gt;
  &lt;thead&gt;
    &lt;tr style=&#34;text-align: right;&#34;&gt;
      &lt;th&gt;&lt;/th&gt;
      &lt;th&gt;incident_id&lt;/th&gt;
      &lt;th&gt;date&lt;/th&gt;
      &lt;th&gt;state&lt;/th&gt;
      &lt;th&gt;city_or_county&lt;/th&gt;
      &lt;th&gt;address&lt;/th&gt;
      &lt;th&gt;n_killed&lt;/th&gt;
      &lt;th&gt;n_injured&lt;/th&gt;
      &lt;th&gt;incident_url&lt;/th&gt;
      &lt;th&gt;source_url&lt;/th&gt;
      &lt;th&gt;incident_url_fields_missing&lt;/th&gt;
      &lt;th&gt;...&lt;/th&gt;
      &lt;th&gt;notes&lt;/th&gt;
      &lt;th&gt;participant_age&lt;/th&gt;
      &lt;th&gt;participant_age_group&lt;/th&gt;
      &lt;th&gt;participant_gender&lt;/th&gt;
      &lt;th&gt;participant_relationship&lt;/th&gt;
      &lt;th&gt;participant_status&lt;/th&gt;
      &lt;th&gt;participant_type&lt;/th&gt;
      &lt;th&gt;sources&lt;/th&gt;
      &lt;th&gt;state_house_district&lt;/th&gt;
      &lt;th&gt;state_senate_district&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;th&gt;0&lt;/th&gt;
      &lt;td&gt;479374&lt;/td&gt;
      &lt;td&gt;1/21/13&lt;/td&gt;
      &lt;td&gt;Louisiana&lt;/td&gt;
      &lt;td&gt;New Orleans&lt;/td&gt;
      &lt;td&gt;LaSalle Street and Martin Luther King Jr. Boul...&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;http://www.gunviolencearchive.org/incident/479374&lt;/td&gt;
      &lt;td&gt;http://www.nola.com/crime/index.ssf/2013/01/no...&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;Unprovoked drive-by results in multiple teens ...&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0::Male||1::Male||2::Male||3::Male||4::Male&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0::Injured||1::Injured||2::Injured||3::Injured...&lt;/td&gt;
      &lt;td&gt;0::Victim||1::Victim||2::Victim||3::Victim||4:...&lt;/td&gt;
      &lt;td&gt;http://www.huffingtonpost.com/2013/01/21/new-o...&lt;/td&gt;
      &lt;td&gt;93.0&lt;/td&gt;
      &lt;td&gt;5.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;1&lt;/th&gt;
      &lt;td&gt;479603&lt;/td&gt;
      &lt;td&gt;2/9/13&lt;/td&gt;
      &lt;td&gt;Louisiana&lt;/td&gt;
      &lt;td&gt;New Orleans&lt;/td&gt;
      &lt;td&gt;400 block of Bourbon Street&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;http://www.gunviolencearchive.org/incident/479603&lt;/td&gt;
      &lt;td&gt;http://www.nola.com/crime/index.ssf/2013/04/su...&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0::18||1::22||2::21||3::29||4::19||5::22||6::23&lt;/td&gt;
      &lt;td&gt;0::Adult 18+||1::Adult 18+||2::Adult 18+||3::A...&lt;/td&gt;
      &lt;td&gt;0::Male||1::Female||2::Female||3::Male||4::Mal...&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0::Injured||1::Injured||2::Injured||3::Injured...&lt;/td&gt;
      &lt;td&gt;0::Victim||1::Victim||2::Victim||3::Victim||4:...&lt;/td&gt;
      &lt;td&gt;http://www.cbsnews.com/news/mardi-gras-shootin...&lt;/td&gt;
      &lt;td&gt;93.0&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;2&lt;/th&gt;
      &lt;td&gt;486209&lt;/td&gt;
      &lt;td&gt;5/12/13&lt;/td&gt;
      &lt;td&gt;Louisiana&lt;/td&gt;
      &lt;td&gt;New Orleans&lt;/td&gt;
      &lt;td&gt;Frenchmen Street&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;19&lt;/td&gt;
      &lt;td&gt;http://www.gunviolencearchive.org/incident/486209&lt;/td&gt;
      &lt;td&gt;http://www.nola.com/crime/index.ssf/2015/09/mo...&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;ms; 19 inj. Rival gang members retaliation eff...&lt;/td&gt;
      &lt;td&gt;17::10||18::10||19::19||20::23&lt;/td&gt;
      &lt;td&gt;0::Adult 18+||1::Adult 18+||2::Adult 18+||3::A...&lt;/td&gt;
      &lt;td&gt;0::Male||1::Male||2::Male||3::Male||4::Male||5...&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0::Injured||1::Injured||2::Injured||3::Injured...&lt;/td&gt;
      &lt;td&gt;0::Victim||1::Victim||2::Victim||3::Victim||4:...&lt;/td&gt;
      &lt;td&gt;http://www.upi.com/Top_News/US/2013/05/12/Abou...&lt;/td&gt;
      &lt;td&gt;97.0&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;3&lt;/th&gt;
      &lt;td&gt;489121&lt;/td&gt;
      &lt;td&gt;6/23/13&lt;/td&gt;
      &lt;td&gt;Louisiana&lt;/td&gt;
      &lt;td&gt;New Orleans&lt;/td&gt;
      &lt;td&gt;5700 block of Wickfield Drive&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;4&lt;/td&gt;
      &lt;td&gt;http://www.gunviolencearchive.org/incident/489121&lt;/td&gt;
      &lt;td&gt;http://www.wwltv.com/story/news/crime/2014/09/...&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;ms. 4 inj at going away party for a soldier in...&lt;/td&gt;
      &lt;td&gt;0::18||1::20||2::18||3::18&lt;/td&gt;
      &lt;td&gt;0::Adult 18+||1::Adult 18+||2::Adult 18+||3::A...&lt;/td&gt;
      &lt;td&gt;0::Female||1::Female||2::Male||3::Male||4::Male&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0::Injured||1::Injured||2::Injured||3::Injured...&lt;/td&gt;
      &lt;td&gt;0::Victim||1::Victim||2::Victim||3::Victim||4:...&lt;/td&gt;
      &lt;td&gt;http://www.nola.com/crime/index.ssf/2013/06/fo...&lt;/td&gt;
      &lt;td&gt;97.0&lt;/td&gt;
      &lt;td&gt;3.0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;th&gt;4&lt;/th&gt;
      &lt;td&gt;92153&lt;/td&gt;
      &lt;td&gt;1/1/14&lt;/td&gt;
      &lt;td&gt;Louisiana&lt;/td&gt;
      &lt;td&gt;New Orleans&lt;/td&gt;
      &lt;td&gt;2300 block of North Prieur Street&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;http://www.gunviolencearchive.org/incident/92153&lt;/td&gt;
      &lt;td&gt;http://www.wwltv.com/news/crime/NOPD-reports-s...&lt;/td&gt;
      &lt;td&gt;False&lt;/td&gt;
      &lt;td&gt;...&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0::Adult 18+&lt;/td&gt;
      &lt;td&gt;0::Male&lt;/td&gt;
      &lt;td&gt;NaN&lt;/td&gt;
      &lt;td&gt;0::Killed&lt;/td&gt;
      &lt;td&gt;0::Victim&lt;/td&gt;
      &lt;td&gt;http://www.wwltv.com/news/crime/NOPD-reports-s...&lt;/td&gt;
      &lt;td&gt;93.0&lt;/td&gt;
      &lt;td&gt;4.0&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;5 rows × 28 columns&lt;/p&gt;
&lt;/div&gt;

&lt;ul&gt;
&lt;li&gt;Now, we have a filtered data set of:&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(len(gv_no)) #total number of reports

len(gv_no)/5 #number of years captured in our dataset range
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;3153





630.6
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;observations. So that means from 2013 to 2018, there were roughly 3,153 reports (which equates to roughly 631 per year) of gun violence to some degree in the New Orleans metro area. What does that mean for yearly breakdowns? What of the nature of these reports and what was going on in the country or area during spikes? Lastly, how do these compare to national averages? All of these questions will be the focus of the rest of this report.&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#To plot yearly breakdowns, we need to construct new variables
#first inspect the date variable to see if we need to create it in &#39;datetime&#39;
gv_no.date.dtype
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;dtype(&#39;O&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import datetime
gv_no[&#39;date&#39;] = gv_no[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;)
gv_no.date.dtype
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;dtype(&#39;&amp;lt;M8[ns]&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;lsquo;&amp;lt;M8[ns]&amp;rsquo; implies conversion to readable date type&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#TODO: CREATE DICTIONARIES TO HOUSE YEARLY TOTALS 
...

year_1 = {&#39;killed&#39;:0, &#39;injured&#39;:0, &#39;total&#39;:0, &#39;2013 Average&#39;:0}
year_2 = {&#39;killed&#39;:0, &#39;injured&#39;:0, &#39;total&#39;:0, &#39;2014 Average&#39;:0}
year_3 = {&#39;killed&#39;:0, &#39;injured&#39;:0, &#39;total&#39;:0, &#39;2015 Average&#39;:0}
year_4 = {&#39;killed&#39;:0, &#39;injured&#39;:0, &#39;total&#39;:0, &#39;2016 Average&#39;:0}
year_5 = {&#39;killed&#39;:0, &#39;injured&#39;:0, &#39;total&#39;:0, &#39;2017 Average&#39;:0}
year_6 = {&#39;killed&#39;:0, &#39;injured&#39;:0, &#39;total&#39;:0, &#39;2018 Average&#39;:0}

if 13 in gv_no[&#39;date&#39;] and gv_no[&#39;n_killed&#39;] &amp;gt; 0:
        year_1[&#39;killed&#39;] +1 
        

&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;------------------------------------------------------------------------

ValueError                             Traceback (most recent call last)

&amp;lt;ipython-input-13-8f2429ab7160&amp;gt; in &amp;lt;module&amp;gt;
      6 year_6 = {&#39;killed&#39;:0, &#39;injured&#39;:0, &#39;total&#39;:0, &#39;2018 Average&#39;:0}
      7 
----&amp;gt; 8 if 13 in gv_no[&#39;date&#39;] and gv_no[&#39;n_killed&#39;] &amp;gt; 0:
      9         year_1[&#39;killed&#39;] +1
     10 


~/anaconda3/lib/python3.7/site-packages/pandas/core/generic.py in __nonzero__(self)
   1476         raise ValueError(&amp;quot;The truth value of a {0} is ambiguous. &amp;quot;
   1477                          &amp;quot;Use a.empty, a.bool(), a.item(), a.any() or a.all().&amp;quot;
-&amp;gt; 1478                          .format(self.__class__.__name__))
   1479 
   1480     __bool__ = __nonzero__


ValueError: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(year_1)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;{&#39;killed&#39;: 0       0
1       0
2       0
3       0
4       1
5       0
6       0
7       0
8       0
9       0
10      0
11      0
12      0
13      1
14      0
15      0
16      0
17      0
18      0
19      0
20      0
21      0
22      0
23      0
24      0
25      0
26      0
27      0
28      0
29      0
       ..
3123    1
3124    0
3125    0
3126    0
3127    1
3128    0
3129    0
3130    0
3131    1
3132    0
3133    1
3134    0
3135    0
3136    0
3137    0
3138    0
3139    2
3140    0
3141    0
3142    1
3143    0
3144    1
3145    0
3146    0
3147    0
3148    1
3149    0
3150    0
3151    0
3152    0
Name: n_killed, Length: 3153, dtype: int64, &#39;injured&#39;: 0        5
1        4
2       19
3        4
4        0
5        2
6        0
7        0
8        1
9        1
10       1
11       1
12       1
13       0
14       0
15       1
16       0
17       1
18       0
19       0
20       0
21       1
22       0
23       0
24       1
25       1
26       0
27       0
28       0
29       1
        ..
3123     0
3124     0
3125     0
3126     0
3127     0
3128     0
3129     1
3130     1
3131     0
3132     1
3133     1
3134     1
3135     1
3136     0
3137     1
3138     1
3139     0
3140     1
3141     0
3142     0
3143     0
3144     0
3145     1
3146     2
3147     0
3148     1
3149     1
3150     1
3151     0
3152     1
Name: n_injured, Length: 3153, dtype: int64, &#39;total&#39;: 0, &#39;2013 Average&#39;: 0}
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>The Semi Universal Ml Workflow</title>
      <link>/post/the-semi-universal-ml-workflow/</link>
      <pubDate>Fri, 16 Aug 2019 11:44:22 +0100</pubDate>
      
      <guid>/post/the-semi-universal-ml-workflow/</guid>
      <description>

&lt;h1 id=&#34;noting-the-suggested-ml-workflow-provided-by-francois-chollet-2017&#34;&gt;Noting the suggested ML Workflow, provided by Francois Chollet (2017):&lt;/h1&gt;

&lt;p&gt;These 7 steps are a guaranteed baseline approach to use on any ML project or anlysis:&lt;/p&gt;

&lt;h2 id=&#34;1-define-our-problem&#34;&gt;1) Define our Problem:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;a) What will our data look like? What is our target variable(s)? Will our data be amenable to splitting into training, validation, and test sets?&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;b) What tools will we be using? Binary, multi-class, or multi-label classification? Perhaps scalar regression? Unsupervised learning or dimensionality reduction? This is a function of (a) and an in-depth understanding of our data.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;2-define-success-in-our-problem&#34;&gt;2) Define &amp;lsquo;success&amp;rsquo; in our problem:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;a) What does a successful model look like and how would we assess this? Typically model accuracy through metrics such as ROC-area under curve are useful for classification problems. For regression we might employ MAE metrics.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;3-evaluation-protocol&#34;&gt;3) Evaluation Protocol:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Will we use k-fold cross validation? Will we shuffle data prior to splitting into training and test datasets? Also consider &amp;lsquo;hold-out&amp;rsquo; methods for data and imputing dropout layers in our network.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;4-prepare-our-data&#34;&gt;4) Prepare our data:&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Format data into tensors, typically scaled to appropriate values (i.e. -1 - 1, or 0 - 1)&lt;/li&gt;
&lt;li&gt;Normalize features that take many different forms or distributions&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&#34;5-training-data-baseline-model&#34;&gt;5) Training Data, Baseline Model:&lt;/h2&gt;

&lt;p&gt;Main choices here:
- a) What will our last activation layer look like? remember that for binary classification, a final &amp;lsquo;sigmoid&amp;rsquo; layer with 1 hidden unit would be appropriate, whereas a scalar regression problem should contain NO final layer of activation, and our multi-classifcation problem required a final layer (&amp;lsquo;softmax&amp;rsquo;) of n-units depending on the features&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;b) What loss function should we use? Binary classification = &amp;lsquo;binary_crossentropy&amp;rsquo;; regression = &amp;lsquo;mse&amp;rsquo;; etc.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;c) What optimization measure will be employed to assess the model learning rate? Usually &amp;lsquo;rmsprop&amp;rsquo; is sufficient.&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;*keep in mind that your hypothesis here is that your model will predict better&lt;/p&gt;

&lt;h2 id=&#34;6-get-big&#34;&gt;6) Get big!&lt;/h2&gt;

&lt;ul&gt;
&lt;li&gt;Here, we will incrementally scale up our model, meaning a few key things:

&lt;ul&gt;
&lt;li&gt;adding layers&lt;/li&gt;
&lt;li&gt;adding units in those layers&lt;/li&gt;
&lt;li&gt;train over more epochs
&lt;br /&gt;&lt;/li&gt;
&lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Make sure to plot/monitor training loss and validation loss over these increments!&lt;/strong&gt;&lt;/p&gt;

&lt;h2 id=&#34;7-optimize-model-parameters-hyperparameters&#34;&gt;7) Optimize Model Parameters/Hyperparameters:&lt;/h2&gt;

&lt;p&gt;Suggested steps to do iteratively:&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;Add dropout layers (induce random zeroes etc.)&lt;/li&gt;
&lt;li&gt;Implement different architectures, ADD or REMOVE layers&lt;/li&gt;
&lt;li&gt;Try out some different hyperparam&amp;rsquo;s - increase or decrease the units per layer, learning rate of the optimizer etc.&lt;/li&gt;
&lt;li&gt;Consider some feature engineering, or configure the data for features that are useful, logical, or altogether inofrmative&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>Reuters Headline Classification Model</title>
      <link>/post/reuters-headline-classification-model/</link>
      <pubDate>Fri, 16 Aug 2019 11:44:06 +0100</pubDate>
      
      <guid>/post/reuters-headline-classification-model/</guid>
      <description>

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from keras.datasets import reuters
(train_data, train_labels), (test_data, test_labels) = reuters.load_data(num_words=10000)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Using TensorFlow backend.
WARNING: Logging before flag parsing goes to stderr.
W0714 18:24:28.310750 4580939200 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

W0714 18:24:28.311494 4580939200 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.AttrValue is deprecated. Please use tf.compat.v1.AttrValue instead.

W0714 18:24:28.312218 4580939200 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.COMPILER_VERSION is deprecated. Please use tf.version.COMPILER_VERSION instead.

W0714 18:24:28.312906 4580939200 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.CXX11_ABI_FLAG is deprecated. Please use tf.sysconfig.CXX11_ABI_FLAG instead.

W0714 18:24:28.313544 4580939200 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.ConditionalAccumulator is deprecated. Please use tf.compat.v1.ConditionalAccumulator instead.
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;len(train_data)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;8982
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;len(test_data)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;2246
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train_data[5]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[1,
 4,
 37,
 38,
 309,
 213,
 349,
 1632,
 48,
 193,
 229,
 463,
 28,
 156,
 635,
 11,
 82,
 14,
 156,
 635,
 11,
 82,
 54,
 139,
 16,
 349,
 105,
 462,
 311,
 28,
 296,
 147,
 11,
 82,
 14,
 296,
 147,
 11,
 54,
 139,
 342,
 48,
 193,
 3234,
 361,
 122,
 23,
 1332,
 28,
 318,
 942,
 11,
 82,
 14,
 318,
 942,
 11,
 82,
 54,
 139,
 122,
 7,
 105,
 462,
 23,
 349,
 28,
 296,
 767,
 11,
 82,
 14,
 296,
 767,
 11,
 54,
 139,
 342,
 229,
 162,
 7,
 48,
 193,
 55,
 408,
 28,
 258,
 557,
 11,
 82,
 14,
 196,
 557,
 11,
 82,
 54,
 139,
 162,
 7,
 105,
 462,
 55,
 349,
 28,
 191,
 968,
 11,
 82,
 14,
 191,
 785,
 11,
 54,
 139,
 17,
 12]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import numpy as np

def vectorize_sequences(sequences, dimension=10000):
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1.
    return results

# Our vectorized training data
x_train = vectorize_sequences(train_data)
# Our vectorized test data
x_test = vectorize_sequences(test_data)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#one-hot encode our categorical variables, the keras way!
from keras.utils.np_utils import to_categorical

one_hot_train_labels = to_categorical(train_labels)
one_hot_test_labels = to_categorical(test_labels)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from keras import models
from keras import layers

model = models.Sequential()
model.add(layers.Dense(64, activation = &#39;relu&#39;, input_shape=(10000, )))
model.add(layers.Dense(64, activation = &#39;relu&#39;))
model.add(layers.Dense(46, activation = &#39;softmax&#39;)) #46 due 46-dimensional output- this will output a probability function for all dimensional spaces added up to 1
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.compile(optimizer=&#39;rmsprop&#39;,
             loss = &#39;categorical_crossentropy&#39;, #due to the categorical nature of our target labels
             metrics = [&#39;accuracy&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;W0714 18:24:36.805853 4580939200 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x_val = x_train[:1000]
partial_x_train = x_train[1000:]
y_val = one_hot_train_labels[:1000]
partial_y_train = one_hot_train_labels[1000:]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;history = model.fit(partial_x_train,
                   partial_y_train,
                    epochs = 20,
                    batch_size = 512,
                    validation_data = (x_val, y_val))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;W0714 18:24:39.409189 4580939200 deprecation.py:323] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1251: add_dispatch_support.&amp;lt;locals&amp;gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where


Train on 7982 samples, validate on 1000 samples
Epoch 1/20
7982/7982 [==============================] - 1s 139us/step - loss: 2.5322 - acc: 0.4955 - val_loss: 1.7208 - val_acc: 0.6120
Epoch 2/20
7982/7982 [==============================] - 1s 98us/step - loss: 1.4452 - acc: 0.6879 - val_loss: 1.3459 - val_acc: 0.7060
Epoch 3/20
7982/7982 [==============================] - 1s 97us/step - loss: 1.0953 - acc: 0.7651 - val_loss: 1.1708 - val_acc: 0.7430
Epoch 4/20
7982/7982 [==============================] - 1s 98us/step - loss: 0.8697 - acc: 0.8165 - val_loss: 1.0793 - val_acc: 0.7590
Epoch 5/20
7982/7982 [==============================] - 1s 96us/step - loss: 0.7034 - acc: 0.8472 - val_loss: 0.9844 - val_acc: 0.7810
Epoch 6/20
7982/7982 [==============================] - 1s 98us/step - loss: 0.5667 - acc: 0.8802 - val_loss: 0.9411 - val_acc: 0.8040
Epoch 7/20
7982/7982 [==============================] - 1s 98us/step - loss: 0.4581 - acc: 0.9048 - val_loss: 0.9083 - val_acc: 0.8020
Epoch 8/20
7982/7982 [==============================] - 1s 96us/step - loss: 0.3695 - acc: 0.9231 - val_loss: 0.9363 - val_acc: 0.7890
Epoch 9/20
7982/7982 [==============================] - 1s 98us/step - loss: 0.3032 - acc: 0.9315 - val_loss: 0.8917 - val_acc: 0.8090
Epoch 10/20
7982/7982 [==============================] - 1s 97us/step - loss: 0.2537 - acc: 0.9414 - val_loss: 0.9071 - val_acc: 0.8110
Epoch 11/20
7982/7982 [==============================] - 1s 97us/step - loss: 0.2187 - acc: 0.9471 - val_loss: 0.9177 - val_acc: 0.8130
Epoch 12/20
7982/7982 [==============================] - 1s 98us/step - loss: 0.1873 - acc: 0.9508 - val_loss: 0.9027 - val_acc: 0.8130
Epoch 13/20
7982/7982 [==============================] - 1s 97us/step - loss: 0.1703 - acc: 0.9521 - val_loss: 0.9323 - val_acc: 0.8110
Epoch 14/20
7982/7982 [==============================] - 1s 98us/step - loss: 0.1536 - acc: 0.9554 - val_loss: 0.9689 - val_acc: 0.8050
Epoch 15/20
7982/7982 [==============================] - 1s 98us/step - loss: 0.1390 - acc: 0.9560 - val_loss: 0.9686 - val_acc: 0.8150
Epoch 16/20
7982/7982 [==============================] - 1s 99us/step - loss: 0.1313 - acc: 0.9560 - val_loss: 1.0220 - val_acc: 0.8060
Epoch 17/20
7982/7982 [==============================] - 1s 99us/step - loss: 0.1217 - acc: 0.9579 - val_loss: 1.0254 - val_acc: 0.7970
Epoch 18/20
7982/7982 [==============================] - 1s 97us/step - loss: 0.1198 - acc: 0.9582 - val_loss: 1.0430 - val_acc: 0.8060
Epoch 19/20
7982/7982 [==============================] - 1s 96us/step - loss: 0.1138 - acc: 0.9597 - val_loss: 1.0955 - val_acc: 0.7970
Epoch 20/20
7982/7982 [==============================] - 1s 97us/step - loss: 0.1111 - acc: 0.9593 - val_loss: 1.0674 - val_acc: 0.8020
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#viz the accuracy and loss across 20 epochs:
import matplotlib.pyplot as plt

loss = history.history[&#39;loss&#39;]
val_loss = history.history[&#39;val_loss&#39;]

epochs = range(1, len(loss) + 1)

plt.plot(epochs, loss, &#39;bo&#39;, label=&#39;Training loss&#39;)
plt.plot(epochs, val_loss, &#39;b&#39;, label=&#39;Validation loss&#39;)
plt.title(&#39;Training and validation loss&#39;)
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;Loss&#39;)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 640x480 with 1 Axes&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.clf() # clear figure
acc = history.history[&#39;acc&#39;]
val_acc = history.history[&#39;val_acc&#39;]
plt.plot(epochs, acc, &#39;bo&#39;, label=&#39;Training acc&#39;)
plt.plot(epochs, val_acc, &#39;b&#39;, label=&#39;Validation acc&#39;)
plt.title(&#39;Training and validation accuracy&#39;)
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;Loss&#39;)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;reuters-headline-classification-model_files/reuters-headline-classification-model_11_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#looks like at around 9 epochs there&#39;s an asymptote of both model accuracy and loss
#re-train a model with 9 epochs:
model = models.Sequential()
model.add(layers.Dense(64, activation = &#39;relu&#39;, input_shape=(10000, )))
model.add(layers.Dense(64, activation = &#39;relu&#39;))
model.add(layers.Dense(46, activation = &#39;softmax&#39;))

model.compile(optimizer = &#39;rmsprop&#39;,
             loss = &#39;categorical_crossentropy&#39;,
             metrics = [&#39;accuracy&#39;])

model.fit(partial_x_train,
partial_y_train,
epochs=9, #change to 9 epochs from 20
batch_size=512,
validation_data=(x_val, y_val))
results = model.evaluate(x_test, one_hot_test_labels)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Train on 7982 samples, validate on 1000 samples
Epoch 1/9
7982/7982 [==============================] - 1s 124us/step - loss: 2.8046 - acc: 0.5038 - val_loss: 1.8434 - val_acc: 0.6300
Epoch 2/9
7982/7982 [==============================] - 1s 96us/step - loss: 1.5257 - acc: 0.6957 - val_loss: 1.3305 - val_acc: 0.7120
Epoch 3/9
7982/7982 [==============================] - 1s 95us/step - loss: 1.1115 - acc: 0.7710 - val_loss: 1.1392 - val_acc: 0.7470
Epoch 4/9
7982/7982 [==============================] - 1s 97us/step - loss: 0.8719 - acc: 0.8146 - val_loss: 1.0308 - val_acc: 0.7850
Epoch 5/9
7982/7982 [==============================] - 1s 97us/step - loss: 0.7032 - acc: 0.8475 - val_loss: 0.9681 - val_acc: 0.8020
Epoch 6/9
7982/7982 [==============================] - 1s 96us/step - loss: 0.5661 - acc: 0.8753 - val_loss: 0.9420 - val_acc: 0.7980
Epoch 7/9
7982/7982 [==============================] - 1s 95us/step - loss: 0.4567 - acc: 0.9074 - val_loss: 0.9235 - val_acc: 0.7960
Epoch 8/9
7982/7982 [==============================] - 1s 95us/step - loss: 0.3709 - acc: 0.9238 - val_loss: 0.9698 - val_acc: 0.7920
Epoch 9/9
7982/7982 [==============================] - 1s 95us/step - loss: 0.3020 - acc: 0.9361 - val_loss: 0.8983 - val_acc: 0.8150
2246/2246 [==============================] - 0s 61us/step
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[0.979011650076957, 0.7894033837403343]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#verify predictions on novel data
predictions = model.predict(x_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;predictions[0].shape
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;(46,)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.sum(predictions[0])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;1.0000001
&lt;/code&gt;&lt;/pre&gt;

&lt;h1 id=&#34;experiment-with-different-network-layers-and-units&#34;&gt;Experiment with different Network Layers and Units:&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model_128u = models.Sequential()

model_128u.add(layers.Dense(128, activation=&#39;relu&#39;, input_shape=(10000,)))
model_128u.add(layers.Dense(128, activation=&#39;relu&#39;))
model_128u.add(layers.Dense(46, activation = &#39;softmax&#39;))

model_128u.compile(optimizer=&#39;rmsprop&#39;,
                  loss=&#39;categorical_crossentropy&#39;,
                  metrics=[&#39;accuracy&#39;])

hist_128u = model_128u.fit(partial_x_train,
                        partial_y_train,
                        epochs=20,
                        batch_size = 512,
                        validation_data = (x_val, y_val))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Train on 7982 samples, validate on 1000 samples
Epoch 1/20
7982/7982 [==============================] - 1s 144us/step - loss: 2.1379 - acc: 0.5540 - val_loss: 1.3870 - val_acc: 0.6880
Epoch 2/20
7982/7982 [==============================] - 1s 86us/step - loss: 1.0956 - acc: 0.7648 - val_loss: 1.0949 - val_acc: 0.7690
Epoch 3/20
7982/7982 [==============================] - 1s 117us/step - loss: 0.7737 - acc: 0.8322 - val_loss: 0.9575 - val_acc: 0.8150
Epoch 4/20
7982/7982 [==============================] - 1s 118us/step - loss: 0.5631 - acc: 0.8834 - val_loss: 0.9109 - val_acc: 0.8090
Epoch 5/20
7982/7982 [==============================] - 1s 118us/step - loss: 0.4036 - acc: 0.9158 - val_loss: 0.8710 - val_acc: 0.8150
Epoch 6/20
7982/7982 [==============================] - 1s 117us/step - loss: 0.3238 - acc: 0.9315 - val_loss: 0.8674 - val_acc: 0.8220
Epoch 7/20
7982/7982 [==============================] - 1s 117us/step - loss: 0.2422 - acc: 0.9458 - val_loss: 0.8880 - val_acc: 0.8220
Epoch 8/20
7982/7982 [==============================] - 1s 118us/step - loss: 0.2104 - acc: 0.9486 - val_loss: 0.9060 - val_acc: 0.8190
Epoch 9/20
7982/7982 [==============================] - 1s 112us/step - loss: 0.1774 - acc: 0.9518 - val_loss: 0.9197 - val_acc: 0.8100
Epoch 10/20
7982/7982 [==============================] - 1s 117us/step - loss: 0.1621 - acc: 0.9525 - val_loss: 0.9367 - val_acc: 0.8100
Epoch 11/20
7982/7982 [==============================] - 1s 115us/step - loss: 0.1456 - acc: 0.9555 - val_loss: 0.9720 - val_acc: 0.8130
Epoch 12/20
7982/7982 [==============================] - 1s 95us/step - loss: 0.1337 - acc: 0.9578 - val_loss: 1.0050 - val_acc: 0.7970
Epoch 13/20
7982/7982 [==============================] - 1s 118us/step - loss: 0.1357 - acc: 0.9557 - val_loss: 0.9884 - val_acc: 0.8150
Epoch 14/20
7982/7982 [==============================] - 1s 116us/step - loss: 0.1292 - acc: 0.9562 - val_loss: 1.0644 - val_acc: 0.7970
Epoch 15/20
7982/7982 [==============================] - 1s 112us/step - loss: 0.1193 - acc: 0.9573 - val_loss: 1.0271 - val_acc: 0.8030
Epoch 16/20
7982/7982 [==============================] - 1s 117us/step - loss: 0.1165 - acc: 0.9573 - val_loss: 1.0800 - val_acc: 0.7970
Epoch 17/20
7982/7982 [==============================] - 1s 118us/step - loss: 0.1181 - acc: 0.9569 - val_loss: 1.0277 - val_acc: 0.7950
Epoch 18/20
7982/7982 [==============================] - 1s 118us/step - loss: 0.1093 - acc: 0.9572 - val_loss: 1.1501 - val_acc: 0.7920
Epoch 19/20
7982/7982 [==============================] - 1s 117us/step - loss: 0.1085 - acc: 0.9573 - val_loss: 1.0522 - val_acc: 0.8040
Epoch 20/20
7982/7982 [==============================] - 1s 87us/step - loss: 0.1081 - acc: 0.9578 - val_loss: 1.0899 - val_acc: 0.7930
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.clf() # clear figure
acc = hist_128u.history[&#39;acc&#39;]
val_acc = hist_128u.history[&#39;val_acc&#39;]
plt.plot(epochs, acc, &#39;bo&#39;, label=&#39;Training acc&#39;)
plt.plot(epochs, val_acc, &#39;b&#39;, label=&#39;Validation acc&#39;)
plt.title(&#39;Training and validation accuracy&#39;)
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;Loss&#39;)
plt.legend()
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;reuters-headline-classification-model_files/reuters-headline-classification-model_19_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#looks like we see the peak of our asymptote at around 7 epochs:
model_128u = models.Sequential()

model_128u.add(layers.Dense(128, activation=&#39;relu&#39;, input_shape=(10000,)))
model_128u.add(layers.Dense(128, activation=&#39;relu&#39;))
model_128u.add(layers.Dense(46, activation = &#39;softmax&#39;))

model_128u.compile(optimizer=&#39;rmsprop&#39;,
                  loss=&#39;categorical_crossentropy&#39;,
                  metrics=[&#39;accuracy&#39;])

hist_128u = model_128u.fit(partial_x_train,
                        partial_y_train,
                        epochs=7,
                        batch_size = 512,
                        validation_data = (x_val, y_val))

result_128u = model_128u.evaluate(x_test, one_hot_test_labels)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Train on 7982 samples, validate on 1000 samples
Epoch 1/7
7982/7982 [==============================] - 1s 151us/step - loss: 2.1407 - acc: 0.5378 - val_loss: 1.3850 - val_acc: 0.6630
Epoch 2/7
7982/7982 [==============================] - 1s 110us/step - loss: 1.0999 - acc: 0.7590 - val_loss: 1.0990 - val_acc: 0.7700
Epoch 3/7
7982/7982 [==============================] - 1s 117us/step - loss: 0.7843 - acc: 0.8310 - val_loss: 0.9611 - val_acc: 0.7960
Epoch 4/7
7982/7982 [==============================] - 1s 117us/step - loss: 0.5626 - acc: 0.8771 - val_loss: 0.9129 - val_acc: 0.8140
Epoch 5/7
7982/7982 [==============================] - 1s 119us/step - loss: 0.4093 - acc: 0.9154 - val_loss: 0.8941 - val_acc: 0.8050
Epoch 6/7
7982/7982 [==============================] - 1s 104us/step - loss: 0.3111 - acc: 0.9332 - val_loss: 0.9658 - val_acc: 0.7890
Epoch 7/7
7982/7982 [==============================] - 1s 117us/step - loss: 0.2485 - acc: 0.9427 - val_loss: 0.9028 - val_acc: 0.8060
2246/2246 [==============================] - 0s 49us/step
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;result_128u
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[0.9940730629493801, 0.786286731967943]
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Predicting Housing Prices Example</title>
      <link>/post/predicting-housing-prices-example/</link>
      <pubDate>Fri, 16 Aug 2019 11:43:56 +0100</pubDate>
      
      <guid>/post/predicting-housing-prices-example/</guid>
      <description>

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#from page 78 of DL (Chollet)
from keras.datasets import boston_housing

(train_data, train_targets), (test_data, test_targets) = boston_housing.load_data()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Using TensorFlow backend.
WARNING: Logging before flag parsing goes to stderr.
W0715 11:25:49.465377 4610311616 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

W0715 11:25:49.466117 4610311616 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.AttrValue is deprecated. Please use tf.compat.v1.AttrValue instead.

W0715 11:25:49.466736 4610311616 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.COMPILER_VERSION is deprecated. Please use tf.version.COMPILER_VERSION instead.

W0715 11:25:49.467211 4610311616 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.CXX11_ABI_FLAG is deprecated. Please use tf.sysconfig.CXX11_ABI_FLAG instead.

W0715 11:25:49.468060 4610311616 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.ConditionalAccumulator is deprecated. Please use tf.compat.v1.ConditionalAccumulator instead.
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train_data.shape
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;(404, 13)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#this process is called feature-wise normalization:

mean = train_data.mean(axis=0)
train_data -= mean
std = train_data.std(axis=0)
train_data/= std

test_data -= mean
test_data /= std
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This centers the means around 0 and provides a unit standard deviation&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from keras import layers
from keras import models

def build_model():
    model = models.Sequential()
    
    model.add(layers.Dense(64, activation=&#39;relu&#39;,
    input_shape=(train_data.shape[1],)))
    model.add(layers.Dense(64, activation=&#39;relu&#39;))
    model.add(layers.Dense(1))
    
    model.compile(optimizer=&#39;rmsprop&#39;, loss=&#39;mse&#39;, metrics=[&#39;mae&#39;])
    return model
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;1) Note: no activation on this layer- typical for scalar regression problems, this allows our network to learn values of any range (rather than, say &amp;lsquo;sigmoid&amp;rsquo; to predict outputs between 0 and 1)&lt;/p&gt;

&lt;p&gt;2) Note: &amp;lsquo;mae&amp;rsquo; stands for mean absolute error, or the absolute difference between the preds and the targets
- An MAE = 0.5 here means that our predicted house price values would be off by $500&lt;/p&gt;

&lt;h3 id=&#34;in-our-k-fold-cross-validation-we-need-to-be-wise-about-how-we-split-our-training-sets&#34;&gt;In our k-fold cross validation, we need to be wise about how we split our training sets:&lt;/h3&gt;

&lt;p&gt;Since these clusters could be relatively small, we need to group by thpose data with the &lt;strong&gt;highest variance&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
&lt;li&gt;K-fold validation implies splitting our data into some k= number of clusters of data, running K identical models, then spitting out an averaged score of performance across models&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#k val set-up:
import numpy as np

k = 4
num_val_samples = len(train_data) // k
num_epochs = 100
all_scores = []

for i in range(k):
    print(&#39;processing fold #&#39;, i)
    # Prepare the validation data: data from partition # k
    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]
    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]
    
    # Prepare the training data: data from all other partitions
    partial_train_data = np.concatenate(
        [train_data[:i * num_val_samples],
        train_data[(i + 1) * num_val_samples:]],
        axis=0)
    partial_train_targets = np.concatenate(
        [train_targets[:i * num_val_samples],
        train_targets[(i + 1) * num_val_samples:]],
        axis=0)

    # Build the Keras model (already compiled)
    model = build_model()

    # Train the model (in silent mode, verbose=0)
    model.fit(partial_train_data, partial_train_targets,
        epochs=num_epochs, batch_size=1, verbose=0)
    
    # Evaluate the model on the validation data
    val_mse, val_mae = model.evaluate(val_data, val_targets, verbose=0)
    all_scores.append(val_mae)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;W0715 11:28:34.739506 4610311616 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.



processing fold # 0
processing fold # 1
processing fold # 2
processing fold # 3
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;all_scores
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[2.4400245529590268]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;np.mean(all_scores)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;2.4400245529590268
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This means that we are off by about $2,440 in our prediction of price. Not insignificant for housing costs in 1970&amp;hellip; Let&amp;rsquo;s see what we can do to improve:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;num_epochs = 500
all_mae_histories = []

for i in range(k):
    print(&#39;processing fold #&#39;, i)

    # Prepare the validation data: data from partition # k
    val_data = train_data[i * num_val_samples: (i + 1) * num_val_samples]
    val_targets = train_targets[i * num_val_samples: (i + 1) * num_val_samples]

    # Prepare the training data: data from all other partitions
    partial_train_data = np.concatenate(
        [train_data[:i * num_val_samples],
        train_data[(i + 1) * num_val_samples:]],
        axis=0)

    partial_train_targets = np.concatenate(
        [train_targets[:i * num_val_samples],
        train_targets[(i + 1) * num_val_samples:]],
        axis=0)

    # Build the Keras model (already compiled)
    model = build_model()

    # Train the model (in silent mode, verbose=0)
    history = model.fit(partial_train_data, partial_train_targets,
        validation_data=(val_data, val_targets),
        epochs=num_epochs, batch_size=1, verbose=0)
    
    mae_history = history.history[&#39;val_mean_absolute_error&#39;]
    all_mae_histories.append(mae_history)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;processing fold # 0
processing fold # 1
processing fold # 2
processing fold # 3
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;average_mae_history = [
    np.mean([x[i] for x in all_mae_histories]) for i in range(num_epochs)]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;%matplotlib inline  
#plotting mae history:
import matplotlib.pyplot as plt

plt.plot(range(1, len(average_mae_history) + 1), average_mae_history) #plots the length or number of average maes, adds one and continues to plot
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;Val MAE&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;predicting-housing-prices-example_files/predicting-housing-prices-example_13_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def smooth_curve(points, factor=0.9):
    smoothed_points = []

    for point in points:
        if smoothed_points:
            previous = smoothed_points[-1]
            smoothed_points.append(previous * factor + point * (1 - factor))
        else:
            smoothed_points.append(point)

    return smoothed_points

smooth_mae_history = smooth_curve(average_mae_history[10:])

plt.plot(range(1, len(smooth_mae_history) + 1), smooth_mae_history)
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;Validation MAE&#39;)
plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;predicting-housing-prices-example_files/predicting-housing-prices-example_14_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#let&#39;s retrain data:
model = build_model()

#on entire data set:
model.fit(train_data, train_targets,
         epochs = 80,
         batch_size=16, 
         verbose=0)

test_mse_score, test_mae_score = model.evaluate(test_data, test_targets)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;102/102 [==============================] - 0s 329us/step
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;test_mae_score
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;2.9702411632911834
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Off by now almost $3,000! - &lt;em&gt;analysis and modeling effort to be continued&lt;/em&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Jund Transcriptional Factor Binding Site Prediction</title>
      <link>/post/jund-transcriptional-factor-binding-site-prediction/</link>
      <pubDate>Fri, 16 Aug 2019 11:43:38 +0100</pubDate>
      
      <guid>/post/jund-transcriptional-factor-binding-site-prediction/</guid>
      <description>&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import deepChem as dc
import numpy as np
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;---------------------------------------------------------------------------

ModuleNotFoundError                       Traceback (most recent call last)

&amp;lt;ipython-input-1-cbeda8b5e316&amp;gt; in &amp;lt;module&amp;gt;
----&amp;gt; 1 import deepChem as dc
      2 import numpy as np


ModuleNotFoundError: No module named &#39;deepChem&#39;
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Imdb Rating Prediction Models V20</title>
      <link>/post/imdb-rating-prediction-models-v20/</link>
      <pubDate>Fri, 16 Aug 2019 11:43:24 +0100</pubDate>
      
      <guid>/post/imdb-rating-prediction-models-v20/</guid>
      <description>

&lt;h1 id=&#34;predicting-movie-ratings-in-keras-model-tuning&#34;&gt;Predicting Movie Ratings in Keras - Model Tuning:&lt;/h1&gt;

&lt;ul&gt;
&lt;li&gt;We trained a neural network with 4 layers on the Keras IMDB data set in order to predict movie ratings from random training data&lt;/li&gt;

&lt;li&gt;&lt;p&gt;We reached a local minimum of validation loss and model accuracy of 17% and 94%, respectively.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;We posited a change in:&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;1) Number of tensors/layers&lt;/p&gt;

&lt;p&gt;2) Units per tensor&lt;/p&gt;

&lt;p&gt;3) Parameters of model evaluation/activation that may improve our model&amp;rsquo;s predictive power.&lt;/p&gt;

&lt;p&gt;We will address each of these in turn, as fitting our models first will allow subsequent incremental changes &lt;em&gt;(rather than having to re-run models each time, which can get computationally expensive).&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;1-tweaking-hidden-layers&#34;&gt;1)  Tweaking hidden layers:&lt;/h2&gt;

&lt;p&gt;This sounds more exotic than it is. We simply want to add and remove hidden layers to our initial model, monitoring changes in performance along the way.&lt;/p&gt;

&lt;p&gt;First, we need to re-import all of our data and pre-processing steps to ensure reproducibility:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from keras.datasets import imdb

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)

#we need to vectorize our data:
import numpy as np
def vect_seq(sequences, dimension=10000):
    # Create an all-zero matrix of shape (len(sequences), dimension)
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1. # set specific indices of results[i] to 1s
    return results

# Our vectorized training data
x_train = vect_seq(train_data)
# Our vectorized test data
x_test = vect_seq(test_data)

y_train = np.asarray(train_labels).astype(&#39;float32&#39;)
y_test = np.asarray(test_labels).astype(&#39;float32&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Using TensorFlow backend.
WARNING: Logging before flag parsing goes to stderr.
W0714 12:34:16.196576 4322899392 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

W0714 12:34:16.197201 4322899392 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.AttrValue is deprecated. Please use tf.compat.v1.AttrValue instead.

W0714 12:34:16.197646 4322899392 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.COMPILER_VERSION is deprecated. Please use tf.version.COMPILER_VERSION instead.

W0714 12:34:16.198417 4322899392 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.CXX11_ABI_FLAG is deprecated. Please use tf.sysconfig.CXX11_ABI_FLAG instead.

W0714 12:34:16.198928 4322899392 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.ConditionalAccumulator is deprecated. Please use tf.compat.v1.ConditionalAccumulator instead.
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from keras import models
from keras import layers
from keras import optimizers
from keras import losses
from keras import metrics
import matplotlib.pyplot as plt
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;em&gt;Model with n+1 layers:&lt;/em&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mod_3L = models.Sequential()

mod_3L.add(layers.Dense(16, activation=&#39;relu&#39;, input_shape=(10000,)))
mod_3L.add(layers.Dense(16, activation=&#39;relu&#39;))
mod_3L.add(layers.Dense(16, activation=&#39;relu&#39;)) #additional dense hidden layer with 16 units
mod_3L.add(layers.Dense(1, activation=&#39;sigmoid&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#compile:
mod_3L.compile(optimizer=&#39;rmsprop&#39;,
    loss=&#39;binary_crossentropy&#39;,
    metrics=[&#39;accuracy&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;W0714 12:34:28.305108 4322899392 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.

W0714 12:34:28.324115 4322899392 deprecation.py:323] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:182: add_dispatch_support.&amp;lt;locals&amp;gt;.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.
Instructions for updating:
Use tf.where in 2.0, which has the same broadcast rule as np.where
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mod_3L.fit(x_train, y_train, epochs=4, batch_size=512)

results_3L = mod_3L.evaluate(x_test, y_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Epoch 1/4
25000/25000 [==============================] - 2s 80us/step - loss: 0.1382 - acc: 0.9502
Epoch 2/4
25000/25000 [==============================] - 2s 79us/step - loss: 0.1224 - acc: 0.9568
Epoch 3/4
25000/25000 [==============================] - 2s 79us/step - loss: 0.1047 - acc: 0.9622
Epoch 4/4
25000/25000 [==============================] - 2s 79us/step - loss: 0.0892 - acc: 0.9690
25000/25000 [==============================] - 1s 47us/step
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results_3L
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[0.42676588655471803, 0.86348]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mod_3L.predict(x_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;array([[0.0731495 ],
       [0.99997604],
       [0.5084689 ],
       ...,
       [0.09003073],
       [0.01447853],
       [0.7448582 ]], dtype=float32)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Looks like we have a moderate loss of model accuracy, with a wider range of predictive point estimates for individual reviews. We can conclude adding a hidden layer does not likely improve this model.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;1-1-removing-a-hidden-layer&#34;&gt;1.1. Removing a hidden layer:&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mod_1L = models.Sequential()

mod_1L.add(layers.Dense(16, activation=&#39;relu&#39;, input_shape=(10000,))) #just 1 dense, 16 unit layer 
mod_1L.add(layers.Dense(1, activation=&#39;sigmoid&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mod_1L.compile(optimizer=&#39;rmsprop&#39;,
    loss=&#39;binary_crossentropy&#39;,
    metrics=[&#39;accuracy&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mod_1L.fit(x_train, y_train, epochs=4, batch_size=512)

results_1L = mod_1L.evaluate(x_test, y_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Epoch 1/4
25000/25000 [==============================] - 2s 88us/step - loss: 0.1713 - acc: 0.9412
Epoch 2/4
25000/25000 [==============================] - 2s 84us/step - loss: 0.1531 - acc: 0.9486
Epoch 3/4
25000/25000 [==============================] - 2s 84us/step - loss: 0.1388 - acc: 0.9545
Epoch 4/4
25000/25000 [==============================] - 2s 84us/step - loss: 0.1274 - acc: 0.9574
25000/25000 [==============================] - 1s 30us/step
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results_1L
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[0.3138607511281967, 0.8792]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mod_1L.predict(x_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;array([[0.15594196],
       [0.9999641 ],
       [0.70080477],
       ...,
       [0.12436602],
       [0.04793406],
       [0.61118674]], dtype=float32)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;em&gt;Little to no impact to our model accuracy, however removing n=1 dense layer significantly reduced the variance (at least superficially), between the highest and lowest predictive accuracy of our neural net on individual reviews.&lt;/em&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&#34;looks-like-changing-our-layers-has-little-to-no-impact-on-our-overall-model-accuracy-prioritizing-simplicity-lets-stick-to-our-mod-1l-with-one-dense-layer-and-now-tweak-tensor-units&#34;&gt;Looks like changing our layers has little to no impact on our overall model accuracy. Prioritizing simplicity, lets stick to our &amp;lsquo;mod_1L&amp;rsquo; with one dense layer, and now tweak tensor units:&lt;/h3&gt;

&lt;h1 id=&#34;2-unit-per-layer-differentiation&#34;&gt;2) Unit-per-Layer Differentiation:&lt;/h1&gt;

&lt;p&gt;A fancy way to phrase &lt;strong&gt;&amp;lsquo;we will now tweak the number of hidden units will be present in each layer&amp;rsquo;&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mod_1L_32u = models.Sequential()

mod_1L_32u.add(layers.Dense(32, activation=&#39;relu&#39;, input_shape=(10000, )))
mod_1L_32u.add(layers.Dense(1, activation=&#39;sigmoid&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mod_1L_32u.compile(optimizer=&#39;rmsprop&#39;,
                  loss=&#39;binary_crossentropy&#39;,
                  metrics=[&#39;accuracy&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mod_1L_32u.fit(x_train, y_train, epochs=4, batch_size=512)

results_1L_32u = mod_1L_32u.evaluate(x_test, y_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Epoch 1/4
25000/25000 [==============================] - 2s 92us/step - loss: 0.4251 - acc: 0.8314
Epoch 2/4
25000/25000 [==============================] - 2s 86us/step - loss: 0.2520 - acc: 0.9122
Epoch 3/4
25000/25000 [==============================] - 2s 86us/step - loss: 0.1993 - acc: 0.9292
Epoch 4/4
25000/25000 [==============================] - 2s 86us/step - loss: 0.1696 - acc: 0.9422
25000/25000 [==============================] - 1s 32us/step
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results_1L_32u
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[0.2895566564083099, 0.88496]
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;*Interesting. We see some general improvement to our model accuracy (around 2%). We could stop, here, but let&amp;rsquo;s exhaust all possibilities to see if this is just a local maximum of accuracy or &lt;strong&gt;global.&lt;/strong&gt; Can we improve further&amp;hellip;?*&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;2-1-64-unit-hidden-layers&#34;&gt;2.1. 64 Unit Hidden Layers:&lt;/h1&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mod_1L_64u = models.Sequential()

mod_1L_64u.add(layers.Dense(32, activation=&#39;relu&#39;, input_shape=(10000, )))
mod_1L_64u.add(layers.Dense(1, activation=&#39;sigmoid&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mod_1L_64u.compile(optimizer=&#39;rmsprop&#39;,
                  loss=&#39;binary_crossentropy&#39;,
                  metrics=[&#39;accuracy&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mod_1L_64u.fit(x_train, y_train, epochs=4, batch_size=512)

results_1L_64u = mod_1L_64u.evaluate(x_test, y_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Epoch 1/4
25000/25000 [==============================] - 2s 93us/step - loss: 0.4199 - acc: 0.8327
Epoch 2/4
25000/25000 [==============================] - 2s 86us/step - loss: 0.2522 - acc: 0.9114
Epoch 3/4
25000/25000 [==============================] - 2s 86us/step - loss: 0.1995 - acc: 0.9303
Epoch 4/4
25000/25000 [==============================] - 2s 86us/step - loss: 0.1693 - acc: 0.9424
25000/25000 [==============================] - 1s 32us/step
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results_1L_64u
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[0.2949124105834961, 0.88192]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mod_1L_64u.predict(x_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;array([[0.19108787],
       [0.99987483],
       [0.7720454 ],
       ...,
       [0.10865086],
       [0.06636879],
       [0.51513207]], dtype=float32)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;&lt;p&gt;Little different from our previous network. Let&amp;rsquo;s stick to our &amp;lsquo;simpler is better&amp;rsquo; heuristic, thereby sticking to 32 units within 1 hidden layer.&lt;/p&gt;&lt;/li&gt;

&lt;li&gt;&lt;p&gt;Now, we can tweak different activation parameters and monitor changes as before. &lt;strong&gt;Let&amp;rsquo;s do this now:&lt;/strong&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&#34;3-model-compiling-and-activation-parameters&#34;&gt;3) Model Compiling and Activation Parameters:&lt;/h1&gt;

&lt;p&gt;*First, we evaluate our model using a different loss function. To avoid the technical details, this is the function that will specify how we descend down our gradient for tensor weights.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;To be even less technical, changing this parameter changes how our network &amp;lsquo;learns&amp;rsquo; and updates our models.&lt;/strong&gt;&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#chaning our compiler for mod_1L_32u to an &#39;MSE&#39; or mean squared error loss function:
mod_1L_32u.compile(optimizer=&#39;rmsprop&#39;,
                  loss=&#39;mse&#39;,
                  metrics=[&#39;accuracy&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mod_1L_32u.fit(x_train, y_train, epochs=4, batch_size=512)

results_1L_32u_fit2 = mod_1L_32u.evaluate(x_test, y_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Epoch 1/4
25000/25000 [==============================] - 2s 92us/step - loss: 0.0432 - acc: 0.9461
Epoch 2/4
25000/25000 [==============================] - 2s 85us/step - loss: 0.0355 - acc: 0.9585
Epoch 3/4
25000/25000 [==============================] - 2s 85us/step - loss: 0.0326 - acc: 0.9641
Epoch 4/4
25000/25000 [==============================] - 2s 85us/step - loss: 0.0289 - acc: 0.9691
25000/25000 [==============================] - 1s 31us/step
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results_1L_32u_fit2
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[0.09572549975037575, 0.8724]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mod_1L_32u.predict(x_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;array([[0.12358373],
       [0.99981403],
       [0.8463329 ],
       ...,
       [0.2399565 ],
       [0.05467173],
       [0.76879334]], dtype=float32)
&lt;/code&gt;&lt;/pre&gt;

&lt;ul&gt;
&lt;li&gt;We can see a moderate, perhaps negative impact on model performance. &lt;strong&gt;Let&amp;rsquo;s keep tweaking. We will now try an alternative to relu, or &amp;ldquo;rectified linear unit&amp;rdquo; activation.&lt;/strong&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mod_1L_32u_tanh = models.Sequential()

mod_1L_32u_tanh.add(layers.Dense(32, activation=&#39;tanh&#39;, input_shape=(10000, )))
mod_1L_32u_tanh.add(layers.Dense(1, activation=&#39;sigmoid&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mod_1L_32u_tanh.compile(optimizer=&#39;rmsprop&#39;, #return compiler to original form
                  loss=&#39;binary_crossentropy&#39;, 
                  metrics=[&#39;accuracy&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;mod_1L_32u_tanh.fit(x_train, y_train, epochs=4, batch_size=512)

results_1L_32u_fit3 = mod_1L_32u_tanh.evaluate(x_test, y_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Epoch 1/4
25000/25000 [==============================] - 2s 84us/step - loss: 0.1381 - acc: 0.9507
Epoch 2/4
25000/25000 [==============================] - 2s 84us/step - loss: 0.1213 - acc: 0.9587
Epoch 3/4
25000/25000 [==============================] - 2s 84us/step - loss: 0.1087 - acc: 0.9629
Epoch 4/4
25000/25000 [==============================] - 2s 84us/step - loss: 0.0994 - acc: 0.9660
25000/25000 [==============================] - 1s 33us/step
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results_1L_32u_fit3
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[0.4123829656982422, 0.86648]
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&#34;our-final-model-to-predict-movie-reviews-in-the-imdb-database-therefore-is-our-simplest-model-1-dense-hidden-layer-of-32-units-compiled-with-a-binary-cross-entropy-loss-function-and-a-non-linear-relu-or-rectified-linear-unit-activation-function&#34;&gt;&lt;em&gt;Our final model to predict movie reviews in the IMDB database, therefore, is our simplest model: 1 dense hidden layer of 32 units, compiled with a binary cross entropy loss function, and a non-linear (&amp;lsquo;relu&amp;rsquo;, or rectified linear unit) activation function.&lt;/em&gt;&lt;/h2&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;print(&#39;With a final model accuracy of: &#39; + str(results_1L_32u[1]))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;With a final model accuracy of: 0.88496
&lt;/code&gt;&lt;/pre&gt;
</description>
    </item>
    
    <item>
      <title>Imdb Rating Prediction V1</title>
      <link>/post/imdb-rating-prediction-v1/</link>
      <pubDate>Fri, 16 Aug 2019 11:43:07 +0100</pubDate>
      
      <guid>/post/imdb-rating-prediction-v1/</guid>
      <description>

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from keras.datasets import imdb

(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Using TensorFlow backend.
WARNING: Logging before flag parsing goes to stderr.
W0714 11:56:20.828102 4610053568 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.

W0714 11:56:20.828986 4610053568 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.AttrValue is deprecated. Please use tf.compat.v1.AttrValue instead.

W0714 11:56:20.829607 4610053568 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.COMPILER_VERSION is deprecated. Please use tf.version.COMPILER_VERSION instead.

W0714 11:56:20.830371 4610053568 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.CXX11_ABI_FLAG is deprecated. Please use tf.sysconfig.CXX11_ABI_FLAG instead.

W0714 11:56:20.831021 4610053568 deprecation_wrapper.py:118] From /Users/walker/anaconda3/lib/python3.7/site-packages/tensorflow/__init__.py:98: The name tf.ConditionalAccumulator is deprecated. Please use tf.compat.v1.ConditionalAccumulator instead.
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#inspect data:
train_data[0]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[1,
 14,
 22,
 16,
 43,
 530,
 973,
 1622,
 1385,
 65,
 458,
 4468,
 66,
 3941,
 4,
 173,
 36,
 256,
 5,
 25,
 100,
 43,
 838,
 112,
 50,
 670,
 2,
 9,
 35,
 480,
 284,
 5,
 150,
 4,
 172,
 112,
 167,
 2,
 336,
 385,
 39,
 4,
 172,
 4536,
 1111,
 17,
 546,
 38,
 13,
 447,
 4,
 192,
 50,
 16,
 6,
 147,
 2025,
 19,
 14,
 22,
 4,
 1920,
 4613,
 469,
 4,
 22,
 71,
 87,
 12,
 16,
 43,
 530,
 38,
 76,
 15,
 13,
 1247,
 4,
 22,
 17,
 515,
 17,
 12,
 16,
 626,
 18,
 2,
 5,
 62,
 386,
 12,
 8,
 316,
 8,
 106,
 5,
 4,
 2223,
 5244,
 16,
 480,
 66,
 3785,
 33,
 4,
 130,
 12,
 16,
 38,
 619,
 5,
 25,
 124,
 51,
 36,
 135,
 48,
 25,
 1415,
 33,
 6,
 22,
 12,
 215,
 28,
 77,
 52,
 5,
 14,
 407,
 16,
 82,
 2,
 8,
 4,
 107,
 117,
 5952,
 15,
 256,
 4,
 2,
 7,
 3766,
 5,
 723,
 36,
 71,
 43,
 530,
 476,
 26,
 400,
 317,
 46,
 7,
 4,
 2,
 1029,
 13,
 104,
 88,
 4,
 381,
 15,
 297,
 98,
 32,
 2071,
 56,
 26,
 141,
 6,
 194,
 7486,
 18,
 4,
 226,
 22,
 21,
 134,
 476,
 26,
 480,
 5,
 144,
 30,
 5535,
 18,
 51,
 36,
 28,
 224,
 92,
 25,
 104,
 4,
 226,
 65,
 16,
 38,
 1334,
 88,
 12,
 16,
 283,
 5,
 16,
 4472,
 113,
 103,
 32,
 15,
 16,
 5345,
 19,
 178,
 32]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;train_labels[0]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;1
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;max([max(sequence) for sequence in train_data])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;9999
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#we need to vectorize our data:
import numpy as np
def vect_seq(sequences, dimension=10000):
    # Create an all-zero matrix of shape (len(sequences), dimension)
    results = np.zeros((len(sequences), dimension))
    for i, sequence in enumerate(sequences):
        results[i, sequence] = 1. # set specific indices of results[i] to 1s
    return results
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;# Our vectorized training data
x_train = vect_seq(train_data)
# Our vectorized test data
x_test = vect_seq(test_data)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x_train[0]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;array([0., 1., 1., ..., 0., 0., 0.])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;y_train = np.asarray(train_labels).astype(&#39;float32&#39;)
y_test = np.asarray(test_labels).astype(&#39;float32&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from keras import models
from keras import layers
model = models.Sequential()
model.add(layers.Dense(16, activation=&#39;relu&#39;, input_shape=(10000,)))
model.add(layers.Dense(16, activation=&#39;relu&#39;))
model.add(layers.Dense(1, activation=&#39;sigmoid&#39;))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#compile the data
#model.compile(optimizer=&#39;rmsprop&#39;,
    #loss=&#39;binary_crossentropy&#39;,
    #metrics=[&#39;accuracy&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from keras import optimizers

model.compile(optimizer=optimizers.RMSprop(lr=0.001),
    loss=&#39;binary_crossentropy&#39;,
    metrics=[&#39;accuracy&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;compiling-our-model-with-custom-parameters&#34;&gt;Compiling our model (with custom parameters):&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;from keras import losses
from keras import metrics

model.compile(optimizer=optimizers.RMSprop(lr=0.001),
    loss=losses.binary_crossentropy,
    metrics=[metrics.binary_accuracy])
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;x_val = x_train[:10000]
partial_x_train = x_train[10000:]
y_val = y_train[:10000]
partial_y_train = y_train[10000:]
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;history = model.fit(partial_x_train,
    partial_y_train,
    epochs=20,
    batch_size=512,
validation_data=(x_val, y_val))
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Train on 15000 samples, validate on 10000 samples
Epoch 1/20
15000/15000 [==============================] - 3s 215us/step - loss: 0.5344 - binary_accuracy: 0.7747 - val_loss: 0.3907 - val_binary_accuracy: 0.8705
Epoch 2/20
15000/15000 [==============================] - 2s 126us/step - loss: 0.3168 - binary_accuracy: 0.8983 - val_loss: 0.3264 - val_binary_accuracy: 0.8717
Epoch 3/20
15000/15000 [==============================] - 2s 120us/step - loss: 0.2334 - binary_accuracy: 0.9249 - val_loss: 0.2858 - val_binary_accuracy: 0.8875
Epoch 4/20
15000/15000 [==============================] - 2s 119us/step - loss: 0.1832 - binary_accuracy: 0.9401 - val_loss: 0.2746 - val_binary_accuracy: 0.8903
Epoch 5/20
15000/15000 [==============================] - 2s 120us/step - loss: 0.1518 - binary_accuracy: 0.9517 - val_loss: 0.3000 - val_binary_accuracy: 0.8805
Epoch 6/20
15000/15000 [==============================] - 2s 119us/step - loss: 0.1223 - binary_accuracy: 0.9635 - val_loss: 0.2894 - val_binary_accuracy: 0.8855
Epoch 7/20
15000/15000 [==============================] - 2s 120us/step - loss: 0.1069 - binary_accuracy: 0.9656 - val_loss: 0.3041 - val_binary_accuracy: 0.8848
Epoch 8/20
15000/15000 [==============================] - 2s 119us/step - loss: 0.0857 - binary_accuracy: 0.9753 - val_loss: 0.3232 - val_binary_accuracy: 0.8788
Epoch 9/20
15000/15000 [==============================] - 2s 119us/step - loss: 0.0693 - binary_accuracy: 0.9827 - val_loss: 0.3418 - val_binary_accuracy: 0.8801
Epoch 10/20
15000/15000 [==============================] - 2s 119us/step - loss: 0.0594 - binary_accuracy: 0.9847 - val_loss: 0.3804 - val_binary_accuracy: 0.8773
Epoch 11/20
15000/15000 [==============================] - 2s 121us/step - loss: 0.0461 - binary_accuracy: 0.9899 - val_loss: 0.3959 - val_binary_accuracy: 0.8739
Epoch 12/20
15000/15000 [==============================] - 2s 119us/step - loss: 0.0375 - binary_accuracy: 0.9927 - val_loss: 0.4292 - val_binary_accuracy: 0.8779
Epoch 13/20
15000/15000 [==============================] - 2s 118us/step - loss: 0.0295 - binary_accuracy: 0.9947 - val_loss: 0.4603 - val_binary_accuracy: 0.8683
Epoch 14/20
15000/15000 [==============================] - 2s 123us/step - loss: 0.0247 - binary_accuracy: 0.9956 - val_loss: 0.4999 - val_binary_accuracy: 0.8745
Epoch 15/20
15000/15000 [==============================] - 2s 118us/step - loss: 0.0210 - binary_accuracy: 0.9955 - val_loss: 0.5153 - val_binary_accuracy: 0.8703
Epoch 16/20
15000/15000 [==============================] - 2s 117us/step - loss: 0.0118 - binary_accuracy: 0.9991 - val_loss: 0.5965 - val_binary_accuracy: 0.8665
Epoch 17/20
15000/15000 [==============================] - 2s 118us/step - loss: 0.0124 - binary_accuracy: 0.9984 - val_loss: 0.5746 - val_binary_accuracy: 0.8690
Epoch 18/20
15000/15000 [==============================] - 2s 121us/step - loss: 0.0077 - binary_accuracy: 0.9994 - val_loss: 0.7067 - val_binary_accuracy: 0.8470
Epoch 19/20
15000/15000 [==============================] - 2s 119us/step - loss: 0.0054 - binary_accuracy: 0.9998 - val_loss: 0.6341 - val_binary_accuracy: 0.8669
Epoch 20/20
15000/15000 [==============================] - 2s 118us/step - loss: 0.0072 - binary_accuracy: 0.9987 - val_loss: 0.6585 - val_binary_accuracy: 0.8656
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;history_dict = history.history

history_dict.keys()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;dict_keys([&#39;val_loss&#39;, &#39;val_binary_accuracy&#39;, &#39;loss&#39;, &#39;binary_accuracy&#39;])
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;training-and-validation-loss&#34;&gt;Training and Validation loss:&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;import matplotlib.pyplot as plt

acc = history.history[&#39;binary_accuracy&#39;]
val_acc = history.history[&#39;val_binary_accuracy&#39;]
loss = history.history[&#39;loss&#39;]
val_loss = history.history[&#39;val_loss&#39;]
epochs = range(1, len(acc) + 1)

# &amp;quot;bo&amp;quot; is for &amp;quot;blue dot&amp;quot;
plt.plot(epochs, loss, &#39;bo&#39;, label=&#39;Training loss&#39;)

# b is for &amp;quot;solid blue line&amp;quot;
plt.plot(epochs, val_loss, &#39;b&#39;, label=&#39;Validation loss&#39;)

plt.title(&#39;Training and validation loss&#39;)
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;Loss&#39;)
plt.legend()

plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;&amp;lt;Figure size 640x480 with 1 Axes&amp;gt;
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;training-and-validation-accuracy&#34;&gt;Training and Validation Accuracy:&lt;/h3&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;plt.clf() # clears prior figure 
acc_values = history_dict[&#39;binary_accuracy&#39;]
val_acc_values = history_dict[&#39;val_binary_accuracy&#39;]

plt.plot(epochs, acc, &#39;bo&#39;, label=&#39;Training acc&#39;)
plt.plot(epochs, val_acc, &#39;b&#39;, label=&#39;Validation acc&#39;)
plt.title(&#39;Training and validation accuracy&#39;)
plt.xlabel(&#39;Epochs&#39;)
plt.ylabel(&#39;Loss&#39;)
plt.legend()

plt.show()
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&lt;img src=&#34;imdb-rating-prediction-v1_files/imdb-rating-prediction-v1_19_0.png&#34; alt=&#34;png&#34; /&gt;&lt;/p&gt;

&lt;p&gt;This comparison is a primary exampe of overfitting. Look at the difference of how metrics of accuracy and loss peak around the fourth epoch.&lt;/p&gt;

&lt;p&gt;An appropriate response could be to simply stop our fitting of training data at 4 epochs. Let&amp;rsquo;s do this now:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;#train new model from scratch
model = models.Sequential()

model.add(layers.Dense(16, activation=&#39;relu&#39;, input_shape=(10000,)))
model.add(layers.Dense(16, activation=&#39;relu&#39;))
model.add(layers.Dense(1, activation=&#39;sigmoid&#39;))

model.compile(optimizer=&#39;rmsprop&#39;,
    loss=&#39;binary_crossentropy&#39;,
    metrics=[&#39;accuracy&#39;])

model.fit(x_train, y_train, epochs=4, batch_size=512)

results = model.evaluate(x_test, y_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;Epoch 1/4
25000/25000 [==============================] - 2s 82us/step - loss: 0.4749 - acc: 0.8217
Epoch 2/4
25000/25000 [==============================] - 2s 74us/step - loss: 0.2666 - acc: 0.9094
Epoch 3/4
25000/25000 [==============================] - 2s 74us/step - loss: 0.1985 - acc: 0.9294
Epoch 4/4
25000/25000 [==============================] - 2s 74us/step - loss: 0.1680 - acc: 0.9405
25000/25000 [==============================] - 2s 77us/step
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;results
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;[0.32515703952789304, 0.87256]
&lt;/code&gt;&lt;/pre&gt;

&lt;h3 id=&#34;now-we-would-like-to-implement-our-model-using-new-data&#34;&gt;Now, we would like to implement our model using new data:&lt;/h3&gt;

&lt;ul&gt;
&lt;li&gt;What are the likelihoods of generating positive reviews? Use &amp;lsquo;predict&amp;rsquo; function from Keras:&lt;/li&gt;
&lt;/ul&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;model.predict(x_test)
&lt;/code&gt;&lt;/pre&gt;

&lt;pre&gt;&lt;code&gt;array([[0.13455817],
       [0.99970734],
       [0.2682383 ],
       ...,
       [0.07024318],
       [0.0425705 ],
       [0.47154084]], dtype=float32)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This shows a wide range of prediction accuracy- from 0.13 to upwards of 0.99.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;What else can we do to improve?&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&#34;next-steps&#34;&gt;&lt;em&gt;Next steps:&lt;/em&gt;&lt;/h2&gt;

&lt;p&gt;1) We could use a different number of hidden layers (in this example we trained 2 layers)
2) Our layers could contain a different number of &amp;lsquo;units&amp;rsquo;
3) Our loss function could be re-tooled- using &amp;lsquo;mse&amp;rsquo; rather than binary cross entropy
4) We could implement a different method of lahyer activation. We have been using &amp;lsquo;relu&amp;rsquo; but what of other methods, such as the common &amp;lsquo;tanh&amp;rsquo; activation function?&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Let&amp;rsquo;s be good scientists and experiment with all (holding other variables equal), with the explicit goal of improving our model accuracy and efficiency.&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&#34;note-model-tuning-to-be-accomplished-in-volume-2-of-this-notebook&#34;&gt;&lt;em&gt;NOTE:  MODEL TUNING TO BE ACCOMPLISHED IN VOLUME 2 OF THIS NOTEBOOK.&lt;/em&gt;&lt;/h1&gt;
</description>
    </item>
    
    <item>
      <title>J. Walker Blackston, aspiring data scientist, goes back to school (again)</title>
      <link>/post/2019-06-05-walker-goes-back-to-school-again/</link>
      <pubDate>Wed, 05 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-06-05-walker-goes-back-to-school-again/</guid>
      <description>&lt;p&gt;Today, I kicked off my first day of class and independent study through MIT&amp;rsquo;s open courseware, &amp;ldquo;Introduction to Computer Science and Programming using Python.&amp;rdquo; I have done some modest prep work to this end, having already podcasted the first five lectures on long rides to the gym, but with how I learn maths and logic, this kind of familiarity is almost necessary.&lt;/p&gt;

&lt;p&gt;We are covering the philosophical underpinnings of computer science and thinking algorithmically. Thinking of an algorithm as a recipe that is repeated has to the the best analogy I have heard to date.&lt;/p&gt;

&lt;p&gt;Next, we performed an exercise to break down the algorithm embedded in the &amp;ldquo;square&amp;rdquo; function in mathematics. It posits that if you have some guess, &amp;ldquo;g,&amp;rdquo; and multiply g by itself to get some result, &amp;ldquo;x,&amp;rdquo; you can repeatedly walk through a process of guessing, adding the guess to the guess divided by our x (so g + x/g) then divide that by 2 (average it) and come increasingly close to the desired outcome. Don&amp;rsquo;t believe me? Try it with the assumption that we do not know the square root of 16. Maybe we start with 3. 3 times 3 is 9&amp;hellip; which ain&amp;rsquo;t 16. Keep going! 3.5? Almost there, but this comes out to 12.25. Again, not 16. As we see our guesses converge nd get closer to 16 we can STOP! Bam. That&amp;rsquo;s an algorithm that we can even represent on our computers. My next task will be to try and programmatically define my own square root function (hopefully by tomorrow!).&lt;/p&gt;

&lt;p&gt;A bit of meta: I have also arrived to a morning schedule that is productive and mindful enough for me to be sustainable. It starts by 1) waking up before 6:30, 2)either jumping straight into a coding challenge (after sufficient caffeine restoration) or workout, 3) a few laps around Audubon park on bike while working on the coding challenges in my head, 4) at least 10 minutes sitting meditation, and 5) an ice cold shower (it&amp;rsquo;s hot here y&amp;rsquo;all, even by 8 am). I like this approach for several reasons: First, the park is quiet and I am the most mentally sharp in the mornings. Also, even if I tank the rest of my day looking at reddit or dream jobs, I can feel good knowing my morning packed alot of devoted deep work in.&lt;/p&gt;

&lt;p&gt;Here goes nothin&amp;rsquo;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>J. Walker Blackston, python noob, writes first own function that works, immediately creates a mistake</title>
      <link>/post/2019-05-27-walker-gets-humbled/</link>
      <pubDate>Mon, 27 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-05-27-walker-gets-humbled/</guid>
      <description>&lt;p&gt;It only took a few hours before I realized the foolishness of my mistake. As I mentioned last, I forgot to save my successful code to run a D&amp;amp;D dice simulator that would print advantage or disadvantage rolls. I am now refactoring it with a tutorial I remember from deep in the ether of other simple python tutorials. I will highlight where this differs from my function. This function brought to you by O&amp;rsquo;Reilly Python cookbooks:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import random

def die(num, sides):    #num equals number of die, sides equal sides of each
    return reduce(lambda x, y, s=sides: x + random.randrange(s),
        range(num+1)) + num
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;There are a couple of things to note. 1) This code is almost embarassingly simple and allows me to see how much work I have left to truly understand writing good code, and 2) it makes use of some functions I simply do not have stored in my mental roledex for use as needed.&lt;/p&gt;

&lt;p&gt;My first attempt had me thinking I would need to initialize the length of the dice, forgetting that the random module would allow a dice to roll and choose a random number from a given range. Then I realized that my input and main argument for the dice() function were the same, causing roll to be undefined in the global space but defined locally as a simple input() string. I also first thought that creating a disadvantaged roll and advantaged roll would require separate functions, now I see that it can be included as one succinct if-then statement. Adding this to the function above, we get:&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;import random
import numpy as np
from functools import reduce

def dice(num, sides):
    def accumulate(x, y, s=sides): return x + random.randrange(s)
    return reduce(accumulate, range(num+1)) + num

print(&#39;How many dice are you rolling?&#39;)
valid_die = [&#39;1&#39;, &#39;2&#39;]
num = []
die = input()
    
if die in valid_die:
    num = int(die)
else: 
    print(&#39;Please enter a valid number of die&#39;)

print(&#39;How many sides per die?&#39;)
valid_sides = [&#39;4&#39;, &#39;6&#39;, &#39;8&#39;, &#39;20&#39;]
sides = []
side_per = input()
    
if side_per in valid_sides:
    sides = int(side_per)
else:
    print(&#39;Please enter valid number of sides per die&#39;)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;This provides a much more definitive program, although I still think it could be simplified and refactored because the function itself only returns one number from a dice roll.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>J. Walker Blackston, python and board game novice, tries Dungeons and Dragons</title>
      <link>/post/2019-05-26-walker-plays-dd/</link>
      <pubDate>Sun, 26 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-05-26-walker-plays-dd/</guid>
      <description>&lt;p&gt;Today I played roughly 4+ hours of what became my first Dungeons and Dragons campaign. High school me rolls his eyes, but he can shove it because I am both smarter and stronger than he is.&lt;/p&gt;

&lt;p&gt;In all seriousness, I see the appeal. The genuine joy you experience when hilarious shit happens to you or your buddies ios rarely duplicated. I don&amp;rsquo;t expect to be swapping my gym membership for a D4 games budget any time soon (a local D&amp;amp;D hangout), but I&amp;rsquo;ll be damned if I didn&amp;rsquo;t have a blast and will continue with this campaign occasionally.&lt;/p&gt;

&lt;p&gt;Importantly, this experience brought me an idea for my first &amp;ldquo;personal project&amp;rdquo; to write in python. I realized this when thinking about how traits and turns are structured. To the un-churched, nearly everything in D&amp;amp;D revolves around a series of multi-sided die. A few are 20-sided (or &amp;ldquo;d20&amp;rdquo;), where others are d6 or d4 etc., all with equal probability of rolling. These rolls dictate player traits, attack abilities, and any other special attributes related to your character arc.&lt;/p&gt;

&lt;p&gt;One condition I&amp;rsquo;d like to play with here is the concept of &amp;ldquo;advantaged&amp;rdquo; or &amp;ldquo;disadvantaged&amp;rdquo; rolls. This simply means that when you roll 2 d20&amp;rsquo;s, you take the lower of the rolls. If you were creating a D&amp;amp;D Python game, how would you simulate these rolls and make a decision?&lt;/p&gt;

&lt;p&gt;&amp;rdquo;&amp;rsquo; #let r_1 and r_2 be the set of possible rolls
r_1 = list(range(1, 20, 1)
r_2 = list(range(1, 20, 1)
#create a function that will take in the move of both, draw randomly with equal probability for all numbers, and make a simple decision to give advantage:&lt;/p&gt;

&lt;p&gt;import random
import numpy as np
def die():&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;print(&#39;Please roll: &#39;)
roll = input()

if roll != &#39;roll&#39;:
    print(&#39;Please type &amp;quot;roll&amp;quot; to roll die&#39;) 

else:
    roll_one = list(range(1, 21))
    roll_two = list(range(1, 21))

    if roll_one &amp;gt; roll_two:
        print(&#39;You rolled a &#39; + str(roll_one) + &#39; and &#39; + str(roll_two) + &#39;. &#39; &#39;Advantage: &#39; + str(roll_one))
        roll = roll_one
        return roll_one 

    elif roll_one &amp;lt; roll_two:
        print(&#39;Disadvantage: &#39; + str(roll_two))
        roll = roll_two
        return roll_two   
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;&amp;rdquo;&amp;rsquo;
I got the first iteration of this to run! Holy cow that felt cool. But then I got cheeky and decided to refactor. Due to some pointers from a professional, I decided to simplify and clean it up, which represents the code you see above. This has taught me several lessons: 1) always save checkpoints of successful code, and 2) if it ain&amp;rsquo;t broke, don&amp;rsquo;t fix it. So now I will be refactoring &lt;em&gt;this&lt;/em&gt; code to make it run again.&lt;/p&gt;

&lt;p&gt;Here goes nothin&amp;rsquo;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>J. Walker Blackston, aspiring data scientist, Designs summer curriculum</title>
      <link>/post/2019-05-25-walker-designs-curriculum/</link>
      <pubDate>Sat, 25 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-05-25-walker-designs-curriculum/</guid>
      <description>&lt;p&gt;Last time we spoke, I was trying to grasp how functions worked, failing miserably to get a web scraper up and running, and suffering from the general malaise of an over-educated mid-20&amp;rsquo;s male who stares at his phone too much.&lt;/p&gt;

&lt;p&gt;Beyond the more existential concerns, I have decided (with the help of my lovely girlfriend) to design a mini summer course of self-study. Thanks to MIT&amp;rsquo;s open courseware, I will take my series of intros to computation and computer science (with a focus in python), algorithms, and potentially some dedicated software developmennt coursework. While many seem to disagree about the best approach for learning something like programming, I will go with the prevailing wisdom that simpler is better in this regard. Lectures, exams, paper, pencil, and whiteboards- the hard and boring way. I will limit my use of a browser as that can be distracting. This may not directly feed into the didactic goals of my PhD, but it will be important that I sharpen these skills for the eventual job search in data science (if academic epidemiology doesn&amp;rsquo;t take shape).&lt;/p&gt;

&lt;p&gt;By the end of June, I hope to have completed 6.0001 and 6.0002, with the goal of more concrete theoretical understanding of how programs work.&lt;/p&gt;

&lt;p&gt;Here goes nothin&amp;rsquo;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>J. Walker Blackston, aspring data scientist, Struggles with functions for cleaning data</title>
      <link>/post/2019-05-23-walker-blackston-tries-more-functions/</link>
      <pubDate>Thu, 23 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-05-23-walker-blackston-tries-more-functions/</guid>
      <description>&lt;p&gt;Well, this morning I worked through a few guided phases of DataQuest&amp;rsquo;s Python For Data Science project which seeks to analyze app data from the Google Play and iOS Apple Stores.&lt;/p&gt;

&lt;p&gt;The project, while outwardly simple, contains many steps that are common to data science. To successfully import and clean our data, we needed to create some personalized functions that can handle our specific needs. In this case, it was the need to identify non-English characters (based on ASCII indexing) and scrub duplicates from our data at the same time. We made some really handy functions for this, the first being a data exploration tool that will provide spaces between some highlighted rows and count/print the number of columns and rows in the set:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def explore_data(dataset, start, end, rows_cols = False):
    data_slice = dataset[start:end]
    for row in data_slice:
        print(row)
        print(&#39;\n&#39;)

    if rows_cols:
        print(&#39;Number of rows:&#39;, len(dataset))
        print(&#39;Number of columns:&#39;, len(dataset[0]))
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Then we wrote a short program that will detect duplicates in data. I know there are likely better functions out there for this, but it was cool to create my own and see what the computer reads through to do so:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;dup_rows = [] #initialize empty lists
unique_rows = []

for dat in data:
  name = dat[0]
  if name in unique_rows:
    dup_rows.append(name)
  else:
    unique_row.append(name)
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which will be updated to include an &amp;lsquo;is_english&amp;rsquo; function to detect non-ASCII characters in a string:&lt;/p&gt;

&lt;pre&gt;&lt;code class=&#34;language-python&#34;&gt;def is_english(string):
    non_am = 0

    for char in string:
        if ord(character) &amp;gt; 127:
            non_am += 1

    if non_am &amp;gt; 3:
        return False
    else:
        return True
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;Which is also just super neat because it uses the built-in function, &amp;lsquo;ord()&amp;rsquo; to tell us the index of characters passed into the string parameter, iterate over however many characters are in the string, and spit out a counter of non-ASCII values. In general, if a string contains more than 3 non-ASCII it&amp;rsquo;s apparently considered non-English. Now I know!&lt;/p&gt;

&lt;p&gt;Anyhow, this was super hard to do without peaking, and I think I know why. I still cannot intuit the way that functions can and should handle variables you pass in. For example, when the dup_rows.append(name) command is specified, I wanted to write it initially with the parameter &amp;lsquo;dat&amp;rsquo; because that&amp;rsquo;s the iterator. But the purpose of the function is to take the iterator to craft a new variable that was originally empty. This will just take practice I guess :/&lt;/p&gt;

&lt;p&gt;Here goes nothin&amp;rsquo;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>J. Walker Blackston, utter programming novice, Starts learning python for data science</title>
      <link>/post/2019-05-21-python_lessons/</link>
      <pubDate>Tue, 21 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-05-21-python_lessons/</guid>
      <description>&lt;p&gt;As I am still launching this website and learning what it does (and more often, does not do) I am reminded of the vast interconnectedness of programming languages.
No language can truly be learned in isolation&amp;hellip;&lt;/p&gt;

&lt;p&gt;Take this blog for example. In order for me to demonstrate what I learn, I need to understand how markdown will process my commands. I have to learn its basics before I can demonstrate myself learning the basics of python with in-line code. Confused yet?&lt;/p&gt;

&lt;p&gt;At any rate, this post is really a procrastination technique for my next chapter of &amp;ldquo;Automate the Boring Stuff&amp;rdquo; on Lists.&lt;/p&gt;

&lt;p&gt;Here goes nothin&amp;rsquo;&lt;/p&gt;

&lt;p&gt;Post mortem: I just worked on python functions that can return desired parameters 1) to the exclusion of the other parameters, accomplished through an &amp;lsquo;if-else&amp;rsquo; code block, and 2) that return statements can feature ALL possible parameters of the function. I also learned about the importance of setting defaults to your arguments when a function will be used repeatedly for the same data or object.&lt;/p&gt;

&lt;p&gt;Post post mortem: It turns out its very difficult to add google analytics and comments into your blog, so that will be a learning lesson. That also caused the delay in this post (the posts directory wouldn&amp;rsquo;t commit because of faulty code).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>J. Walker Blackston, utter programming novice, launches website</title>
      <link>/post/2019-05-19-walker-blackston-site-launched/</link>
      <pubDate>Sun, 19 May 2019 00:00:00 +0000</pubDate>
      
      <guid>/post/2019-05-19-walker-blackston-site-launched/</guid>
      <description>&lt;p&gt;Well.
Finally got around to putting this old website together. Neat thing about it - powered by &lt;a href=&#34;http://jekyllrb.com&#34; target=&#34;_blank&#34;&gt;Jekyll&lt;/a&gt; and I can use Markdown to author my posts. It actually is a lot easier than I thought it was going to be.&lt;/p&gt;

&lt;p&gt;Here goes nothin&amp;rsquo;!&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
